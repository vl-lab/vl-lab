<!DOCTYPE html>
<html lang="en"><head>
	<link rel="icon" href="favicon.ico?v20191121" type="image/x-icon" />
	<meta HTTP-EQUIV="EXPIRES" CONTENT="0">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="Akai and Hank">
    <title>Vision and Learning Lab</title>

    <!-- Bootstrap core CSS -->
    <link href="./vendor/bootstrap/css/bootstrap.min.css?v20191121" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="./vendor/font-awesome/css/font-awesome.min.css?v20191121" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Kaushan+Script" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css">

    <!-- Custom styles for this template -->
    <link  href="css/agency.css?v20191121" rel="stylesheet">
    <link  href="./css/style.css?v20191121" rel="stylesheet">

  </head>

  <body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">Vision and Learning Lab</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fa fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav text-uppercase ml-auto">
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#about">About</a>
			</li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#news">News</a>
			</li>      
<!--
	    <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#portfolio">Research</a>
            </li>
-->

            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#team">Team</a>
            </li>
			
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#Alumni">Alumni</a>
            </li>
			
            <li class="nav-item"> 
              <a class="nav-link js-scroll-trigger" href="#publications">Publications</a>
            </li>
			
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#contact">Contact</a>
            </li>
			
            <li class="nav-item"> 
              <a class="nav-link js-scroll-trigger" href="#recruit">實驗室徵才</a>
            </li>
			
          </ul>
        </div>
      </div>
    </nav>

    <!-- Header -->
    <header class="masthead" id="mastheadbg">
      <div class="container">
       
        <div class="intro-text">
		<br><br><br>
          <div class="intro-lead-in">Welcome To Vision and Learning Lab!</div>
<!--          <div class="intro-heading text-uppercase">It's Nice To Meet You</div>-->
<!--          <div id="clock"></div>-->
          <a class="btn btn-primary btn-xl text-uppercase js-scroll-trigger" href="#about">About us</a>
        </div>
      </div>
    </header>
	
    <!-- About -->
    <section id="about">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Vision and Learning Lab</h2>
            <p class="text-muted" style="text-align: justify;font-size: large">The Vision and Learning Laboratory (VLLab), formerly named Computer Vision Laboratory (CVLab), was founded in the Research Center for Information Technology Innovation, Academia Sinica in January 2011, under the supervision of Dr. Yen-Yu Lin. Since August 2019, this laboratory was relocated to the Department of Computer Science, National Chiao Tung University. We dedicate ourselves to the development of cutting-edge technologies for computer vision, machine learning, pattern recognition, deep learning, and artificial intelligence.</p>
         <iframe src="photoslide.html?v20191121" width="70%" height="500px" frameborder="0" scrolling="no"></iframe> 
          </div>
        </div>
        <br>
      </div>
    </section>
    
    <!-- News -->
    <section class="bg-light" id="news">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">RECENT NEWS</h2><br>
          </div>
        </div>
 		<div class="row">
			<div class="[ col-xs-12 col-sm-offset-2 col-sm-11 ]" style="margin: 0 auto">
				<ul class="event-list" style="width: 110%">
				
					<li><time datetime="2020-07-31">
							<span class="day">31</span>
							<span class="month">JUL</span>
							<span class="year">2020</span>
							<span class="time">Evening</span></time>
						<div class="info"><div class="row"><div class="col-sm-12">
							<p style="font-size: 15px"><strong>Congratulations! Our paper was accepted by MM 2020</strong><br>
							Zhenyu Wu, Duc Hoang, Shih-Yao Lin, Yusheng Xie, Liangjian Chen, Yen-Yu Lin, Zhangyang Wang, and Wei Fan, "MM-Hand: 3D-Aware Multi-Modal Guided Hand Generation for 3D Hand Pose Synthesis," ACM International Conference on Multimedia (MM), October 2020.
							</a></p>							
							</div></div></div></li>				

					<li><time datetime="2020-07-31">
							<span class="day">31</span>
							<span class="month">JUL</span>
							<span class="year">2020</span>
							<span class="time">Evening</span></time>
						<div class="info"><div class="row"><div class="col-sm-12">
							<p style="font-size: 15px"><strong>Congratulations! Our paper was accepted by MM 2020</strong><br>
							Jen-Chun Lin, Wen-Li Wei, Yen-Yu Lin, Tyng-Luh Liu, and Hong-Yuan Mark Liao, "Learning From Music to Visual Storytelling of Shots: A Deep Interactive Learning Mechanism," ACM International Conference on Multimedia (MM), October 2020.
							</a></p>							
							</div></div></div></li>

					<li><time datetime="2020-07-09">
							<span class="day">9</span>
							<span class="month">JUL</span>
							<span class="year">2020</span>
							<span class="time">Evening</span></time>
						<div class="info"><div class="row"><div class="col-sm-12">
							<p style="font-size: 15px"><strong>Congratulations! Our paper was accepted by ECCV 2020</strong><br>
							Cheng-Chun Hsu, Yi-Hsuan Tsai, Yen-Yu Lin, and Ming-Hsuan Yang, "Every Pixel Matters: Center-aware FeatureAlignment for Domain Adaptive Object Detector," European Conference on Computer Vision (ECCV), August 2020.
							</a></p>							
							</div></div></div></li>

					<li><time datetime="2020-04-24">
							<span class="day">24</span>
							<span class="month">APR</span>
							<span class="year">2020</span>
							<span class="time">Evening</span></time>
						<div class="info"><div class="row"><div class="col-sm-12">
							<p style="font-size: 15px"><strong>Congratulations! Our paper was accepted by TPAMI</strong><br>
							Yun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, and Jia-Bin Huang, "Show, Match and Segment: Joint Weakly Supervised Learning of Semantic Matching and Object Co-segmentation," IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).
							</a></p>							
							</div></div></div></li>
				
					<li><time datetime="2020-04-20">
							<span class="day">20</span>
							<span class="month">APR</span>
							<span class="year">2020</span>
							<span class="time">Evening</span></time>
						<div class="info"><div class="row"><div class="col-sm-12">
							<p style="font-size: 15px"><strong>Congratulations! Our paper was accepted by IJCAI 2020</strong><br>
							Han-Yi Lin, Pi-Cheng Hsiu, Tei-Wei Kuo, and Yen-Yu Lin, "Spatiotemporal Super-Resolution with Cross-Task Consistency and Its Semi-supervised Extension," International Joint Conference on Artificial Intelligence (IJCAI), July 2020.
							</a></p>							
							</div></div></div></li>
				
					<li><time datetime="2019-11-13">
							<span class="day">13</span>
							<span class="month">Nov</span>
							<span class="year">2019</span>
							<span class="time">Evening</span></time>
						<div class="info"><div class="row"><div class="col-sm-12">
							<p style="font-size: 15px"><strong>Congratulations! Our paper was accepted by AAAI 2020</strong><br>
							Yung-Han Huang, Kuang-Jui Hsu, Shyh-Kang Jeng, and Yen-Yu Lin, "Weakly-supervised Video Re-localization with Multiscale Attention Model," AAAI Conference on Artificial Intelligence (AAAI), February 2020.
							</a></p>							
							</div></div></div></li>
				
					<li><time datetime="2019-11-13">
							<span class="day">13</span>
							<span class="month">Nov</span>
							<span class="year">2019</span>
							<span class="time">Evening</span></time>
						<div class="info"><div class="row"><div class="col-sm-12">
							<p style="font-size: 15px"><strong>Congratulations! Our paper was accepted by WACV 2020</strong><br>
							Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Yen-Yu Lin, Xiaohui Xie, and Wei Fan, "DGGAN: Depth-image Guided Generative Adversarial Networks for Disentangling RGB and Depth Images for 3D Hand Pose Estimation," IEEE Winter Conference on Applications of Computer Vision (WACV), March 2020.
							</a></p>							
							</div></div></div></li>			
				
					<li><time datetime="2019-9-4">
							<span class="day">4</span>
							<span class="month">Sep</span>
							<span class="year">2019</span>
							<span class="time">Evening</span></time>
						<div class="info"><div class="row"><div class="col-sm-12">
							<p style="font-size: 15px"><strong>Congratulations! Our paper was accepted by NeurIPS 2019</strong><br>
							Cheng-Chun Hsu*, Kuang-Jui Hsu*, Chung-Chi Tsai, Yen-Yu Lin, and Yung-Yu Chuang, "Weakly Supervised Instance Segmentation using the Bounding Box Tightness Prior," Thirty-third Conference on Neural Information Processing Systems (NeurIPS), December 2019.
							</a></p>							
							</div></div></div></li>

					<li><time datetime="2019-8-23">
							<span class="day">23</span>
							<span class="month">Aug</span>
							<span class="year">2019</span>
							<span class="time">Evening</span></time>
						<div class="info"><div class="row"><div class="col-sm-12">
							<p style="font-size: 15px"><strong>Congratulations! Our paper was accepted by IJCV 2019</strong><br>
							Yi-Wen Chen, Yi-Hsuan Tsai, Yen-Yu Lin, and Ming-Hsuan Yang, "VOSTR: Video Object Segmentation via Transferable Representations," to appear in International Journal of Computer Vision (IJCV).
							</a></p>							
							</div></div></div></li>

					<li><time datetime="2019-8-20">
							<span class="day">20</span>
							<span class="month">Aug</span>
							<span class="year">2019</span>
							<span class="time">Evening</span></time>
						<div class="info"><div class="row"><div class="col-sm-12">
							<p style="font-size: 15px"><strong>Congratulations! Our paper was accepted by ICCV 2019</strong><br>
							Yu-Jhe Li, Yun-Chun Chen, Yen-Yu Lin, Xiaofei Du, and Yu-Chiang Frank Wang, "Recover and Identify: A Generative Dual Model for Cross-Resolution Person Re-Identification," IEEE International Conference on Computer Vision (ICCV), October 2019.  
							</a></p>							
							</div></div></div></li>
							
					<li><time datetime="2019-8-20">
							<span class="day">20</span>
							<span class="month">Aug</span>
							<span class="year">2019</span>
							<span class="time">Evening</span></time>
						<div class="info"><div class="row"><div class="col-sm-12">
							<p style="font-size: 15px"><strong>Congratulations! Our paper was accepted by TMM 2019</strong><br>
							Chung-Chi Tsai, Kuang-Jui Hsu, Yen-Yu Lin, Xiaoning Qian, and Yung-Yu Chuang, "Deep Co-saliency Detection via Stacked Autoencoder-enabled Fusion and Self-trained CNNs," to appear in IEEE Transactions on Multimedia (TMM). 
							</a></p>							
							</div></div></div></li>
							
				</ul>
			</div>
     	</div>
     	
<!--     	2009->2016    -->
    <div class="panel-group" id="accordion">
		<div class="panel panel-default">
		  <div class="panel-heading">
			<h4 class="panel-title">
			  <a data-toggle="collapse" data-parent="#accordion" href="#collapse1" style="color: #282626">More...</a>
			</h4>
		  </div>
		  <div id="collapse1" class="panel-collapse collapse in">
			<div class="panel-body">
				<table class="table" width="80%">
				
					<tr>
					 <th>2019/07/19</th> 
					 <td><strong>Congratulations! One paper was accepted by AVSS 2019</strong>
					 <br>Wen-Li Wei, Jen-Chun Lin, Yen-Yu Lin, and Hong-Yuan Mark Liao, "What Makes You Look Like You: Learning an Inherent Feature Representation for Person Re-Identification," has been accepted for AVSS 2019.</td>
					</tr>
					
					<tr>
					 <th>2019/07/19</th> 
					 <td><strong>Congratulations! One paper was accepted by BMVC 2019
					 </strong>
					 <br>Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Hui Tang, Yufan Xue, Yen-Yu Lin, Xiaohui Xie, and Wei Fan, "TAGAN: Tonality Aligned Generative Adversarial Networks for Realistic Hand Pose Synthesis," has been accepted for BMVC 2019.</td>
					</tr>
					
					<tr>
					 <th>2019/07/19</th> 
					 <td><strong>Congratulations! One paper was accepted by BMVC 2019
					 </strong>
					 <br>Yi-Wen Chen, Yi-Hsuan Tsai, Tiantian Wang, Yen-Yu Lin, and Ming-Hsuan Yang, "Referring Expression Object Segmentation with Caption-Aware Consistency," has been accepted for BMVC 2019.</td>
					</tr>
					
					<tr>
					 <th>2019/06/16</th> 
					 <td><strong>Congratulations! One paper will be published in IEEE Trans. on Image Processing
					 </strong>
					 <br>Kuang-Jui Hsu, Yen-Yu Lin, and Yung-Yu Chuang! Our work entitled "Weakly Supervised Salient Object Detection by Learning A Classifier-Driven Map Generator," has been accepted for IEEE Trans. on Image Processing (TIP), volume 28, number 11, pages 5435-5449, November 2019.</td>
					</tr>
					
					<tr>
					 <th>2019/04/14</th> 
					 <td><strong>Congratulations! Yi-Tung Liao won IICM best master thesis award.
					 </strong>
					 <br>Hyperlink (IICM best master thesis award): http://www.iicm.org.tw/award/paper2018.pdf</td>
					</tr>
					
					<tr>
					 <th>2019/02/26</th> 
					 <td><strong>Congratulations! Three paper were accepted by CVPR 2019
					 </strong>
					 <br>Congratulations to Tsun-Yi Yang, Yi-Ting Chen, Yen-Yu Lin, and Yung-Yu Chuang! Our work entitled "FSA-Net: Learning Fine-Grained Structure Aggregation for Head Pose Estimation from a Single Image" has been accepted for CVPR 2019.
					 <br><br>Congratulations to Yun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, and Jia-Bin Huang! Our work entitled "CrDoCo: Pixel-level Domain Transfer with Cross-Domain Consistency" has been accepted for CVPR 2019.
					 <br><br>Congratulations to Kuang-Jui Hsu, Yen-Yu Lin, and Yung-Yu Chuang! Our work entitled "DeepCO3: Deep Instance Co-segmentation by Co-peak Search and Co-saliency" has been accepted for CVPR 2019.
					 </td>
					</tr>
					
					<tr>
					 <th>2019/02/21</th> 
					 <td><strong>Congratulations! One paper was accepted by DAC 2019
					 </strong>
					 <br>Congratulations to Yu-Chuan Chang, Wei-Ming Chen, Pi-Cheng Hsiu, Yen-Yu Lin, and Tei-Wei Kuo! Our work entitled "LSIM: Ultra Lightweight Similarity Measurement for Mobile Graphics Applications" has been accepted for DAC 2019.</td>
					</tr>
					
					<tr>
					 <th>2018/11/07</th> 
					 <td><strong>Congratulations! One paper was accepted by AAAI 2019
					 </strong>
					 <br>Congratulations to Yu-Lun Liu, Yi-Tung Liao, Yen-Yu Lin, and Yung-Yu Chuang! Our work entitled "Deep Video Frame Interpolation using Cyclic Frame Generation" has been accepted for AAAI 2019.</td>
					</tr>
					
					<tr>
					 <th>2018/10/05</th> 
					 <td><strong>Congratulations! Two papers were accepted by ACCV 2018
					 </strong>
					 <br>Congratulations to Yun-Chun Chen, Po-Hsiang Huang, Li-Yu Yu, Jia-Bin Huang, Ming-Hsuan Yang, and Yen-Yu Lin! Our work entitled "Deep Semantic Matching with Foreground Detection and Cycle-Consistency" has been accepted for ACCV 2018.
					 <br><br>Yi-Wen Chen, Yi-Hsuan Tsai, Chu-Ya Yang, Yen-Yu Lin, and Ming-Hsuan Yang! Our work entitled "Unseen Object Segmentation in Videos via Transferable Representations" has been accepted for ACCV 2018.
					 </td>
					</tr>

					<tr>
					 <th>2018/07/23</th> 
					 <td><strong>Congratulations! One paper will be published in IEEE Trans. on Image Processing
					 </strong>
					 <br>Congratulations to Chung-Chi Tsai, Weizhi Li, Kuang-Jui Hsu, Xiaoning Qian, and Yen-Yu Lin! Our work entitled "Image Co-saliency Detection and Co-segmentation via Progressive Joint Optimization" has been accepted for IEEE Trans. on Image Processing (TIP).
					 </td>
					</tr>
					
					<tr>
					 <th>2018/07/04</th> 
					 <td><strong>Congratulations! One papers was accepted by ECCV 2018
					 </strong>
					 <br>Congratulations to Kuang-Jui Hsu, Chung-Chi Tsai, Yen-Yu Lin, Xiaoning Qian, and Yung-Yu Chuang! Our work entitled "Unsupervised CNN-based Co-saliency Detection with Graphical Optimization" has been conditionally accepted for ECCV 2018.
					 </td>
					</tr>
					
					<tr>
					 <th>2018/07/04</th> 
					 <td><strong>Congratulations! Two papers were accepted by BMVC 2018
					 </strong>
					 <br>Congratulations to Wei-Chih Hung, Yi-Hsuan Tsai, Yan-Ting Liou, Yen-Yu Lin, and Ming-Hsuan Yang! Our work entitled "Adversarial Learning for Semi-supervised Semantic Segmentation" has been accepted for BMVC 2018.
					 <br><br>Congratulations to Shih-Yao Lin and Yen-Yu Lin! Our work entitled "Augmenting Mo-cap Data with Neural Data Translation for Human Action Recognition" has been accepted for BMVC 2018.
					 </td>
					</tr>
					
					<tr>
					 <th>2018/07/04</th> 
					 <td><strong>Congratulations! Dr. Yen-Yu Lin receives The Young Scholars' Creativity Award 2018 from Foundation for the Advancement of Outstanding Scholarship
					 </strong>

					 </td>
					</tr>
					
					<tr>
					 <th>2018/06/13</th> 
					 <td><strong>Congratulations! One paper will be published in IEEE Trans. on Image Processing
					 </strong>
					 <br>Congratulations to Chun-Rong Huang, Wei-Cheng Wang, Wei-An Wang, Szu-Yu Lin, and Yen-Yu Lin! Our work entitled "USEQ: Ultra-Fast Superpixel Extraction via Quantization"  has been accepted for IEEE Trans. on Image Processing (TIP).
					 </td>
					</tr>
					
					<tr>
					 <th>2018/06/06</th> 
					 <td><strong>Congratulations! One paper was accepted by ICIP 2018
					 </strong>
					 <br>Congratulations to Wei-Cheng Wang, Hsin-Wei Cheng, Chun-Rong Huang, and Yen-Yu Lin! Our work entitled "Clustering Trajectories in Heterogeneous Representations for Video Event Detection" has been accepted for ICIP 2018.
					 </td>
					</tr>

					<tr>
					 <th>2018/04/27</th> 
					 <td><strong>Congratulations! Dr. Yen-Yu Lin receives the Exploration Research Award 2018 from the Pan Wen-Yuan Foundation
					 </strong>
					 
					 </td>
					</tr>
					
					<tr>
					 <th>2018/04/23</th> 
					 <td><strong>Congratulations! Two papers were accepted by IJCAI 2018
					 </strong>
					 <br>Congratulations to Kuang-Jui Hsu, Yen-Yu Lin, and Yung-Yu Chuang! Our work entitled "Co-attention CNNs for Unsupervised Object Co-segmentation" has been conditionally accepted for IJCAI 2018.
					 <br>
					 <br>Congratulations to Tsun-Yi Yang, Yi-Hsuan Huang, Yen-Yu Lin, Pi-Cheng Hsiu, and Yung-Yu Chuang! Our work entitled "SSR-Net: A Compact Soft Stagewise Regression Network for Age Estimation" has been accepted for IJCAI 2018.
					 </td>
					</tr>
					
					<tr>
					 <th>2018/04/23</th> 
					 <td><strong>Congratulations! Ya-Fang Shih won IICM best master thesis award.
					 </strong>
					 <br>Hyperlink (IICM best master thesis award): http://www.iicm.org.tw/award/paper2017.pdf
					 </td>
					</tr>
					
					<tr>
					 <th>2017/11/13</th> 
					 <td><strong>Congratulations! One paper was accepted by AAAI 2018
					 </strong>
					 <br>Congratulations to Ting-Kuei Hu, Yen-Yu Lin, and Pi-Cheng Hsiu! Our work entitled "Learning Adaptive Hidden Layers for Mobile Gesture Recognition" has been accepted for AAAI 2018.
					 </td>
					</tr>
					
					<tr>
					 <th>2017/07/18</th> 
					 <td><strong>Congratulations! One paper was accepted by ICCV 2017
					 </strong>
					 <br>Congratulations to Tsun-Yi Yang, Jo-Han Hsu, Yen-Yu Lin, and Yung-Yu Chuang! Our work entitled "DeepCD: Learning Deep Complementary Descriptors for Patch Representations" has been accepted for ICCV 2017.
					 </td>
					</tr>
					
					<tr>
					 <th>2017/07/07</th> 
					 <td><strong>Congratulations! One paper was accepted by BMVC 2017
					 </strong>
					 <br>Congratulations to Kuang-Jui Hsu, Yen-Yu Lin, and Yung-Yu Chuang! Our work entitled "Weakly Supervised Saliency Detection with A Category-Driven Map Generator" has been accepted for BMVC 2017.
					 </td>
					</tr>
					
				</table>
			</div>
		  </div>
		</div> 
  	</div> 
	
      </div>
    </section>
    

<!-- Projects Projects Projects Projects Projects Projects -->
<!--
    <section id="portfolio">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Research</h2>
            <h3 class="section-subheading text-muted"></h3>
          </div>
        </div>
        <div class="row">
           <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#ActionRecognition">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fa fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/ar.jpg" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>Action Recognition</h4>
            </div>
          </div>
         <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#MultimediaRetrieval">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fa fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/ir.jpg" width="98%" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>Content-based Multimedia Retrieval</h4>
            </div>
          </div>
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#Classification">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fa fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/cf.jpg" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>Data Classification and Clustering</h4>

            </div>
          </div>
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#FaceDetection">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fa fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/fd.jpg" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>Face Detection</h4>
            </div>
          </div>
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#FeatureMatching">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fa fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="http://cvlab.citi.sinica.edu.tw/images/thumbs/cvpr-chen13.png" width="98%" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>Feature Matching</h4>
            </div>
          </div>
 
           <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#ObjectRecognition">
              <div class="portfolio-hover"><div class="portfolio-hover-content"><i class="fa fa-plus fa-3x"></i></div></div>
              <img class="img-fluid" src="img/portfolio/or.jpg" alt="">
            </a>
            <div class="portfolio-caption"><h4>Object Recognition</h4></div></div>
                          
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#PeopleCounting">
              <div class="portfolio-hover"><div class="portfolio-hover-content"><i class="fa fa-plus fa-3x"></i></div></div>
              <img class="img-fluid" src="img/portfolio/pc.jpg" alt="">
            </a>
            <div class="portfolio-caption"><h4>People Counting</h4></div></div>        
          
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#SemanticSegmentation">
              <div class="portfolio-hover"><div class="portfolio-hover-content"><i class="fa fa-plus fa-3x"></i></div></div>
              <img class="img-fluid" src="img/portfolio/ss.jpg" alt="">
            </a>
            <div class="portfolio-caption"><h4>Semantic Segmentation</h4></div></div>         
          
          
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#TransferLearning">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fa fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/tl.jpg" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>Transfer Learning</h4>
            </div>
          </div>

        </div>
      </div>
    </section>
-->
	<!-- Projects Projects Projects Projects Projects Projects -->
    

    <!-- Team -->
    <!-- ￣￣￣￣￣￣￣￣￣￣￣￣￣￣Member ￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣-->    
    <section id="team">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Our Team</h2>
            <!--            <h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3>-->
          </div>
        </div>
     <!-- Pi -->   

<!-- One Person -->
<h3>Laboratory Director</h3>       
<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<style>
a {
    color:#000000;
}
a:hover{color:#6aa4ab;
</style>
<tr>
<td rowspan="2" valign="middle" width="200"><a target="_blank" href="https://sites.google.com/site/yylinweb/"><img class="mx-auto rounded-circle" src="http://cvlab.citi.sinica.edu.tw/vllab/images/member/yylin.jpg"  border="1" alt="" width="200" /></a></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Dr. Yen-Yu Lin </h4> 
   <p style="margin-top: 5px; margin-bottom: 5px;"><em><span>Ph.D., Department of Computer Science and Information Engineering, National Taiwan University</span></em></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
<li>Machine Learning</li>
<li>Deep Learning</li>
<li>Artificial Intelligence</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: lin[at]cs.nctu.edu.tw </li>
  <li>Tel: +886-3-5712121 ext.54781</li>
  <li></span><img align="top" height="15" src="http://vllab.cs.nctu.edu.tw/images/hyperlink_icons.png" style="font-size:small"><a target="_blank" href="https://sites.google.com/site/yylinweb/" style=hovercolor:#6aa4ab> Website</a> </li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>　</p>

<h3>Research Assistants </h3>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/yblin.jpeg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Yan-Bo Lin </h4> 
 
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>M.S., Graduate Institute of Communication Engineering, National Taiwan University</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><span>2020/08–present</span></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Multimodal Learning with Multi-Modality Data</li>

</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: yblin98[at]ntu.edu.tw </li>
  <li></span><img align="top" height="15" src="http://vllab.cs.nctu.edu.tw/images/hyperlink_icons.png" style="font-size:small"><a target="_blank" href="http://yanbo.nctu.me/" style=hovercolor:#6aa4ab> Website</a> </li>

</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>


<p>　</p>

<h3>Administrative Assistants </h3>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/ctai.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Chiang Tai</h4> 
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>M.F.A., Mount Royal School of Art, Maryland Institute College of Art (MICA)</span></em></p>

  <p style="margin-top: 0px; margin-bottom: 0px;"><span>2020/08–present</span></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Contemporary Art</li>
<li>Interpretation Art</li>
<li>Sculpture and Video Art</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: ctai[at]nctu.edu.tw </li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>



<p>　</p>


<h3>Ph.D. Students</h3>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/hylin.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Han-Yi Lin</h4> 
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>Ph.D. Student, Department of Computer Science and Information Engineering, National Taiwan University</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>Advisors: Dr. Tei-Wei Kuo and Dr. Yen-Yu Lin</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><span>2017/12–present</span></p>
</td>
</tr>
<tr>
<td valign="top" width="40%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Spatial/Temporal Super-Resolution</li>
<li>Video Deblurring</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: d03922006[at]csie.ntu.edu.tw </li>
  <li></span><img align="top" height="15" src="http://vllab.cs.nctu.edu.tw/images/hyperlink_icons.png" style="font-size:small"><a target="_blank" href="https://sites.google.com/view/hanyilin" style="hovercolor:#6aa4ab"> Website</a> </li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>


<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/cky.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Cheng-Kun Yang</h4> 
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>Ph.D. Student, Department of Computer Science and Information Engineering, National Taiwan University</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><span>2019/09–present</span></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
<li>Deep Learning</li>
<li>Medical Imaging</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: d08922002[at]ntu.edu.tw </li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
  <tbody>
  <tr>
  <td>
  <table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
  <tbody>
  <tr>
  <td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/cyho.jpg"  border="1" alt="" width="200" /></td>
  <td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Chia-Yu Ho</h4>
    <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>Ph.D. Student, Institute of Computer Science and Engineering, National Chiao Tung University</span></em></p>
    
	<p style="margin-top: 0px; margin-bottom: 0px;"><span>2020/05–present</span></p>
  </td>
  </tr>
  <tr>
  <td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
    <ul>
  <li>Computer Vision</li>
  <li>Deep Learning</li>
  <li>Reinforcement Learning</li>
  </ul>
  </td>
  <td valign="top" width="70%"><strong>Contact Information:</strong>
    <ul>
    <li>E-mail: mylifeai1116[at]gmail.com </li>
    <li></span><img align="top" height="15" src="http://vllab.cs.nctu.edu.tw/images/hyperlink_icons.png" style="font-size:small"><a target="_blank" href="https://alan1994site.wordpress.com/%e7%b0%a1%e4%bb%8b/" style="hovercolor:#6aa4ab"> Website</a> </li>
  </ul>
  </td>
  </tr>
  </tbody>
  </table>
  </td>
  </tr>
  </tbody>
  </table>


<p>　</p>


<h3>M.S. Students</h3>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/dlwang.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">De-Le Wang</h4> 
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>M.S. Student, Departure of Electrical Engineering, National Tsing Hua University</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>Advisors: Dr. Chia-Wen Lin and Dr. Yen-Yu Lin</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><span>2019/09–present</span></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
<li>Image Processing</li>
<li>Deep Learning</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: alfwang[at]cloud.nthu.edu.tw</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>



<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/frtsai.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Fu-Ren Tsai</h4> 
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>M.S. Student, Institute of Communications Engineering, National Tsing Hua University</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>Advisors: Dr. Chia-Wen Lin and Dr. Yen-Yu Lin</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><span>2019/09–present</span></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Salient Object Detection</li>
<li>Low-Shot Image Segmentation</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: pp00704831[at]yahoo.com.tw</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/kylee.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Kuan-Yi Lee</h4> 
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>M.S. Student, Departure of Electrical Engineering, National Tsing Hua University</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>Advisors: Dr. Chia-Wen Lin and Dr. Yen-Yu Lin</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><span>2020/09–present</span></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
<li>Deep Learning</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: daniel4lee[at]gmail.com</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
  <tbody>
  <tr>
  <td>
  <table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
  <tbody>
  <tr>
  <td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/chcheng.jpg"  border="1" alt="" width="200" /></td>
  <td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Chung-Hsuan Cheng</h4> 
    <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>M.S. Student, Institute of Computer Science and Engineering, National Chiao Tung University</span></em></p>
    
  <p style="margin-top: 0px; margin-bottom: 0px;"><span>2020/03–present</span></p>
  </td>
  </tr>
  <tr>
  <td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
    <ul>
  <li>Computer Vision</li>
    </ul>
  </td>
  <td valign="top" width="70%"><strong>Contact Information:</strong>
    <ul>
    <li>E-mail: cchscott.cs08g[at]nctu.edu.tw</li>
  </ul>
  </td>
  </tr>
  </tbody>
  </table>
  </td>
  </tr>
  </tbody>
  </table>

  
<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/lpsheng.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Po-Sheng Liu</h4> 
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>M.S. Student, Institute of Multimedia Engineering, National Chiao Tung University</span></em></p>
  
  <p style="margin-top: 0px; margin-bottom: 0px;"><span>2020/03–present</span></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: bensonliu0904[at]gmail.com</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/yllu.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Yu-Lin Lu</h4> 
  <p style="margin-top: 5px; margin-bottom: 5px;"><em><span>M.S. Student, Institute of Computer Science and Engineering, National Chiao Tung University</span></em></p>
   <p style="margin-top: 0px; margin-bottom: 0px;"><span>2020/07–present</span></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: ulin010101[at]gmail.com</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/ichung.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">I-Chu Hung</h4> 
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>M.S. Student, Institute of Computer Science and Engineering, National Chiao Tung University</span></em></p>
 
  <p style="margin-top: 0px; margin-bottom: 0px;"><span>2020/07–present</span></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: yijuhung0129[at]gmail.com</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>


<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/hlyen.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Hung-Lin Yen</h4> 
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>M.S. Student, Institute of Computer Science and Engineering, National Chiao Tung University</span></em></p>
  
  <p style="margin-top: 0px; margin-bottom: 0px;"><span>2020/08–present</span></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: henry991843[at]gmail.com</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>



<p>　</p>



        <h3>Undergraduate Students</h3><br>
		<div class="row">	
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="img/team/hcyang.jpg" alt="">
              <h5>Hao-Cheng Yang</h5>hcyang.cs06[at]nctu.edu.tw
              <br>2019/08–present
			  <br>CS, NCTU
			  <br>MOST College Student Research Scholarship: Domain Adaptation Network for 3D Point Cloud Object Classification
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="img/team/hesu.jpg" alt="">
              <h5>Hsin-En Su</h5>littlecrazymouse.cs06[at]nctu.edu.tw
              <br>2020/03–present
			  <br>CS, NCTU
            </div></div>
			
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="img/team/shlu.jpg" alt="">
              <h5>Shao-Hao Lu</h5>kevinlu2240.cs06[at]nctu.edu.tw
              <br>2020/02–present
			  <br>CS, NCTU
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="img/team/yjwu.jpg" alt="">
              <h5>Yun-Jui Wu  </h5>tugddr.cs06[at]nctu.edu.tw
              <br>2020/02–present
			  <br>CS, NCTU
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="img/team/thpeng.jpg" alt="">
              <h5>Tse-Hao Peng   </h5>pength[at]cs.nctu.edu.tw
              <br>2020/02–present
			  <br>CS, NCTU
            </div></div>
            
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="img/team/jjwu.jpg" alt="">
              <h5>Ji-Jia Wu   </h5>jia.cs07[at]nctu.edu.tw
              <br>2020/09–present
			  <br>CS, NCTU
            </div></div>
            
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="img/team/ytliu.jpg" alt="">
              <h5>Yi-Ting Liu</h5>yiting1007.eed07[at]nctu.edu.tw
              <br>2020/09–present
			  <br>CS, NCTU
            </div></div>
            
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="img/team/wjhuang.jpg" alt="">
              <h5>Wei-Jie Huang</h5>weii0000.cs07[at]nctu.edu.tw
              <br>2020/09–present
			  <br>CS, NCTU
            </div></div>
            
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="img/team/pksu.jpg" alt="">
              <h5>Po-Kai Su</h5>a871234342.cs07[at]nctu.edu.tw
              <br>2020/09–present
			  <br>CS, NCTU
            </div></div>
            
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="img/team/kschen.jpg" alt="">
              <h5>Kai-Syun Chen</h5>kaivinnctu.cs07[at]nctu.edu.tw 
              <br>2020/09–present
			  <br>CS, NCTU
            </div></div>
            
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="img/team/jcchiang.jpg" alt="">
              <h5>Jui-Che Chiang</h5>benchiang.cs07[at]nctu.edu.tw 
              <br>2020/09–present
			  <br>CS, NCTU
            </div></div>

        </div>  


<!--
<h3>Undergraduate Students</h3>
<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/hcyang.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Hao-Cheng Yang</h4> 
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>Undergraduate Student, Department of Computer Science, National Chiao Tung University</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>MOST College Student Research Scholarship: Domain Adaptation Network for 3D Point Cloud Object Classification</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><span>2019/08–present</span></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: hcyang.cs06[at]nctu.edu.tw</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/hesu.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Hsin-En Su</h4> 
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>Undergraduate Student, Department of Computer Science, National Chiao Tung University</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><span>2020/03–present</span></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
<li>Deep Learning</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: littlecrazymouse.cs06[at]nctu.edu.tw</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/shlu.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Shao-Hao Lu</h4> 
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>Undergraduate Student, Department of Computer Science, National Chiao Tung University</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><span>2020/02–present</span></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: kevinlu2240.cs06[at]nctu.edu.tw</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/yjwu.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Yun-Jui Wu</h4> 
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>Undergraduate Student, Department of Computer Science, National Chiao Tung University</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><span>2020/02–present</span></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: tugddr.cs06[at]nctu.edu.tw</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/thpeng.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Tse-Hao Peng</h4> 
  <p style="margin-top: 0px; margin-bottom: 0px;"><em><span>Undergraduate Student, Department of Computer Science, National Chiao Tung University</span></em></p>
  <p style="margin-top: 0px; margin-bottom: 0px;"><span>2020/02–present</span></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: pength[at]cs.nctu.edu.tw</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
-->


<p>　</p>

       <div class="row">
	   

          <!--next-->
        </div> 
      <!-- _______________ _______Member______________________-->                                                                 
     <!-- ￣￣￣￣￣￣￣￣￣￣￣￣￣￣Alumni ￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣-->     
    <section id="Alumni">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Alumni</h2>
          </div>
        </div>

	
        <h4>Ph.D.</h4><br>
		<div class="row">
		
<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="images/member/tyyang.jpg" alt="">
<h5>Tsun-Yi Yang</div></td>

<td colspan="2" valign="top" width="1170">
Duration: 2014/09–2019/07<br>
Institute: Computer Science and Information Engineering, National Taiwan University<br>
Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin<br>
Thesis: Deep Feature Learning for Fast and Efficient Regression Application<br>
Next Job: Scape Technologies, UK -> Facebook, UKn<br>
 </span><img align="top" height="15" src="http://vllab.cs.nctu.edu.tw/images/hyperlink_icons.png" style="font-size:small"><a class="readon" href="https://github.com/shamangary" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
</td>
</tr>
</tbody>
</table>
			
<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="images/member/kjhsu.jpg" alt="">
<h5>Kuang-Jui Hsu</div></td>
<td colspan="2" valign="top" width="1170">
Duration: 2014/09–2019/07<br>
Institute: Computer Science and Information Engineering, National Taiwan University<br>
Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin<br>
Thesis: <a class="readon" href="http://vllab.cs.nctu.edu.tw/images/thesis/phdthesis-hsu19.pdf" target="_blank" style="hovercolor:#6aa4ab"><u>Visual Attention-getting Object Discovery via Learning Weakly Supervised CNNs</u></a><br>
<span style="font-size: 15px; line-height: 1.3em; color: #b57843;">Award: Ph.D. Thesis Award, Institute of Information & Computing Machinery (IICM), 2019; Ph.D. Thesis Award, the Chinese Image Processing and Pattern Recognition Society (IPPR), 2019</span><br>
Next Job: Qualcomm, Taiwan<br>
 </span><img align="top" height="15" src="http://vllab.cs.nctu.edu.tw/images/hyperlink_icons.png" style="font-size:small"><a class="readon" href="https://www.linkedin.com/in/kuang-jui-hsu/" target="_blank" style="hovercolor:#6aa4ab">Website</a><br><br>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="images/member/cctsai.jpg" alt="">
<h5>Chung-Chi Tsai</div></td>
<td colspan="2" valign="top" width="1170">
Duration: 2016/05–2018/08<br>
Institute: Electrical Engineering, Texas A&M University<br>
Advisors: Dr. Xiaoning Qian and Dr. Yen-Yu Lin<br>
Thesis: <a class="readon" href="http://vllab.cs.nctu.edu.tw/images/thesis/phdthesis-tsai18.pdf" target="_blank" style="hovercolor:#6aa4ab"><u>Image Co-saliency Detection: Novel Approaches with Convex Optimization and Deep Neural Networks</u></a><br>
Next Job: Qualcomm, US<br>
 </span><img align="top" height="15" src="http://vllab.cs.nctu.edu.tw/images/hyperlink_icons.png" style="font-size:small"><a class="readon" href="https://www.linkedin.com/in/cctsai1/" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="img/team/sylin.jpg" alt="">
<h5>Shih-Yao Lin</div></td>

<td colspan="2" valign="top" width="1170">
Duration: 2014/06–2016/01<br>
Institute: Graduate Institute of Networking and Multimedia, National Taiwan University<br>
Advisors: Dr. Yi-Ping Hung, Dr. Chu-Song Chen, and Dr. Yen-Yu Lin<br>
Thesis: Human Action Recognition based on Probabilistic Graphical Models<br>
<span style="font-size: 15px; line-height: 1.3em; color: #b57843;">Award: Ph.D. Thesis Award, Institute of Information & Computing Machinery (IICM), 2016</span><br>
Next Job: Bosch Research, USA -> Tencent Medical AI Lab, CA, USA<br>
 </span><img align="top" height="15" src="http://vllab.cs.nctu.edu.tw/images/hyperlink_icons.png" style="font-size:small"><a class="readon" href="https://sites.google.com/view/mikelin/" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>

</td>
</tr>
</tbody>
</table>

		
        </div>
			
        <h4>M.S.</h4><br>
		<div class="row">

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="images/member/ytliou.jpg" alt="">
<h5>Yan-Ting Liou
</div></td>
<td colspan="2" valign="top" width="1170">
Duration: 2017/07–2020/08<br>
Institute: Department of Computer Science and Information Engineering, National Taiwan University <br>
Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin<br>
Thesis: <a class="readon" href="http://vllab.cs.nctu.edu.tw/images/thesis/masterthesis-liou20.pdf" target="_blank" style="hovercolor:#6aa4ab"><u>Generalized Fake Image Forensics via Semi-Supervised Anomaly Detection
</u></a><br>
Next Job: Appier
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="img/team/cylin.jpg" alt="">
<h5>Ching-Yuan Lin
</div></td>
<td colspan="2" valign="top" width="1170">
Duration: 2019/03-2020/03<br>
Institute: Department of Electrical Engineering, National Tsing Hua University <br>
Advisors: Dr. Chia-Wen Lin and Dr. Yen-Yu Lin<br>
Thesis: Multiple Foreground Co-segmentation using Unsupervised Convolutional Neural Networks<br>
Next Job: 
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="img/team/yhhuang.jpg" alt="">
<h5>Yi-Hsuan Huang
</div></td>
<td colspan="2" valign="top" width="1170">
Duration: 2018/02–2019/09<br>
Institute: Graduate Institute of Networking and Multimedia, National Taiwan University <br>
Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin<br>
Thesis: OSSN: A One-shot Siamese Network for Semantic Segmentation of 3D Point Clouds<br>
Next Job: MediaTek
</td>
</tr>
</tbody>
</table>
		
<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="images/member/cayang.jpg" alt="">
<h5>Chiao-An Yang 
</div></td>
<td colspan="2" valign="top" width="1170">
Duration: 2017/09–2019/07<br>
Institute: Graduate Institute of Networking and Multimedia, National Taiwan University <br>
Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin<br>
Thesis: Soft Ranking Losses for Deep Image Similarity Estimation<br>
Next Job: Research assistant, National Taiwan University 
</td>
</tr>
</tbody>
</table>
			
<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="images/member/yhhuang_2.jpg" alt="">
<h5>Yung-Han Huang 
</div></td>
<td colspan="2" valign="top" width="1170">
Duration: 2017/09–2019/07<br>
Institute: Data Science Degree Program, NTU & Sinica<br>
Advisors: Dr. Shyh-Kang Jeng and Dr. Yen-Yu Lin<br>
Thesis: Weakly Supervised Video Re-localization of Action Segments with Multiscale Attention Model<br>
Next Job: Perfect Corp.
</td>
</tr>
</tbody>
</table>
			
<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="images/member/ytliao.jpg" alt="">
<h5>Yi-Tung Liao
</div></td>
<td colspan="2" valign="top" width="1170">
Duration: 2016/09–2018/07<br>
Institute: Computer Science and Information Engineering, National Taiwan University<br>
Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin<br>
Thesis: <a class="readon" href="http://vllab.cs.nctu.edu.tw/images/thesis/masterthesis-liao18.pdf" target="_blank" style="hovercolor:#6aa4ab""><u>Deep Video Frame Interpolation using Cyclic Frame Generation</u></a><br>
<span style="font-size: 15px; line-height: 1.3em; color: #b57843;">Award: M.S. Thesis Award, Institute of Information & Computing Machinery (IICM), 2018</span><br>
Next Job: World Quant
</td>
</tr>
</tbody>
</table>
			
<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="images/member/jhhsu.png" alt="">
<h5>Jo-Han Hsu
</div></td>
<td colspan="2" valign="top" width="1170">
Duration: 2015/09–2017/06<br>
Institute: Computer Science and Information Engineering, National Taiwan University<br>
Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin<br>
Thesis: Distilling from the Past: Self-Distillation by Using Epoch Regularization<br>
Next Job: HTC
</td>
</tr>
</tbody>
</table>
			
<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="images/member/yfshih.jpg" alt="">
<h5>Ya-Fang Shih
</div></td>
<td colspan="2" valign="top" width="1170">
Duration: 2016/02–2017/06<br>
Institute: Computer Science and Information Engineering, National Taiwan University<br>
Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin<br>
Thesis: <a class="readon" href="http://vllab.cs.nctu.edu.tw/images/thesis/masterthesis-shih17.pdf" target="_blank" style="hovercolor:#6aa4ab"><u>Deep Co-occurrence Feature Learning for Visual Object Recognition</u></a><br>
<span style="font-size: 15px; line-height: 1.3em; color: #b57843;">Award: M.S. Thesis Award, Institute of Information & Computing Machinery (IICM), 2017; M.S. Thesis Award, the Chinese Image Processing and Pattern Recognition (IPPR) Society, 2017</span><br>
Next Job: HTC -> Verisk Analytics
</td>
</tr>
</tbody>
</table>
			
<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="images/member/yuhu.jpg" alt="">
<h5>Yuan-Ting Hu</div></td>

<td colspan="2" valign="top" width="1170">
Duration: 2013/08–2015/06<br>
Institute: Computer Science and Information Engineering, National Taiwan University<br>
Advisors: Dr. Bing-Yu Chen and Dr. Yen-Yu Lin<br>
Thesis: DescriptorBoost: An Unsupervised Approach to Fusing Multiple Descriptors in the Homography Space<br>
Next Job: ULSee -> Ph.D. Student, Electrical and Computer Engineering, University of Illinois Urbana-Champaign<br>
 </span><img align="top" height="15" src="http://vllab.cs.nctu.edu.tw/images/hyperlink_icons.png" style="font-size:small"><a class="readon" href="https://sites.google.com/view/yuantinghu" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="images/member/hytsai.jpg" alt="">
<h5>Han-Yi Tsai
</div></td>
<td colspan="2" valign="top" width="1170">
Duration: 2013/06–2014/12<br>
Institute: Computer Science and Information Engineering, National Taiwan University<br>
Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin<br>
Thesis: Patch Match with Multiple Descriptors for Scene Alignment<br>
Next Job: Dynacolor
</td>
</tr>
</tbody>
</table>
			
<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="images/member/kjhsu.jpg" alt="">

<h5>Kuang-Jui Hsu</div></td>
<td colspan="2" valign="top" width="1170">
Duration: 2011/04–2013/06<br>
Institute: Graduate Institute of Networking and Multimedia, National Taiwan University <br>
Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin<br>
Thesis: <a class="readon" href="http://vllab.cs.nctu.edu.tw/images/thesis/masterthesis-hsu13.pdf" target="_blank" style="hovercolor:#6aa4ab"><u>Augmented Multiple Instance Regression for Inferring Object Contours within Bounding Boxes</u></a><br>
<span style="font-size: 15px; line-height: 1.3em; color: #b57843;">Award: M.S. Thesis Award, Institute of Information & Computing Machinery (IICM), 2013</span><br>
Next Job: Ph.D. Student, Computer Science and Information Engineering, National Taiwan University<br>
 </span><img align="top" height="15" src="http://vllab.cs.nctu.edu.tw/images/hyperlink_icons.png" style="font-size:small"><a class="readon" href="https://www.linkedin.com/in/kuang-jui-hsu/" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="images/member/ycwang.jpg" alt="">
<h5>Yi-Chen Wang
</div></td>
<td colspan="2" valign="top" width="1170">
Duration: 2011/07–2012/06<br>
Institute: Graduate Institute of Communication Engineering, National Taiwan University<br>
Advisors: Dr. Shyh-Kang Jeng and Dr. Yen-Yu Lin<br>
Thesis: Anomaly Detection Localization via Foreground Motion Information<br>
Next Job:  Taiwan Semiconductor Manufacturing Company
</td>
</tr>
</tbody>
</table>
			
<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="250">
<div class="alumni">
<img class="mx-auto rounded-circle" src="images/member/calin.jpg" alt="">
<h5>Chin-An Lin
</div></td>
<td colspan="2" valign="top" width="1170">
Duration: 2011/07–2012/06<br>
Institute: Graduate Institute of Communication Engineering, National Taiwan University<br>
Advisors: Dr. Shyh-Kang Jeng and Dr. Yen-Yu Lin<br>
Thesis: An Efficient Temporal Model for Action Recognition Using Multivariate Linear Prediction<br>
<span style="font-size: 15px; line-height: 1.3em; color: #b57843;">Award: Best Master Thesis Award, Institute of Information & Computing Machinery (IICM), 2012</span><br>
Next Job: Quanta Research Institute
</td>
</tr>
</tbody>
</table>
			
        </div>

        <h4>Research Assistants</h4><br>
		<div class="row">

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/cchsu.bmp" alt="">
             
             
              <h5>Cheng-Chun Hsu</u></a></h5>2018/07–2020/08
			  <br>Next Job: Research Scientist, PRO Unlimited @ Facebook, UK <br>
<img src="images/hyperlink_icons.png" style="width: 11.5%;height: 11.5%;"><a class="readon" href="https://chengchunhsu.github.io/" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
            </div></div>          
		
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="img/team/hywang.jpg" alt="">
              <h5>Han-Yi Wang </h5>2018/05–2020/07
			  <br>Next Job: M.S. Student, Computer Science & Engineering, Texas A&M University
            </div></div>            
            
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/wcwang.jpg" alt="">
              <h5>Wei-Cheng Wang</u></a></h5>2018/03–2019/07
			  <br>Next Job: Ph.D. Student, Computer Science Engineering, Ghent University<br>
 <img src="images/hyperlink_icons.png" style="width: 11.5%;height: 11.5%;"><a class="readon" href="https://sites.google.com/view/wcw-pws/" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ytchen.jpg" alt="">
              <h5>Yi-Ting Chen</u></a></h5>2018/07–2019/07
			  <br>Next Job: M.S. Student, Master of Science in Computer Vision, Robotics Institute, Carnegie Mellon University<br>
 <img src="images/hyperlink_icons.png" style="width: 11.5%;height: 11.5%;"><a class="readon" href="https://www.linkedin.com/in/yi-ting-chen-b6984694/" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
            </div></div>
			
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ywchen.jpg" alt="">
              <h5>Yi-Wen Chen</u></a></h5>2017/07–2019/06
			  <br>Next Job: Ph.D. Student, Electrical Engineering and Computer Science, University of California, Merced<br>
<img src="images/hyperlink_icons.png" style="width: 11.5%;height: 11.5%;"><a class="readon" href="https://wenz116.github.io/" target="_blank" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/hhhung.jpg" alt="">
              <h5>Hao-Hsiang Hung  </h5>2017/07–2019/03
			  <br>Next Job: M.S. Student, Computer Science and Information Engineering, National Taiwan University
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ccho.jpg" alt="">
              <h5>Chia-Chen Ho   </h5>2018/07–2019/03 
			  <br>Next Job: Research assistant, Computer Science, National Tsing Hua University
            </div></div>
        
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ycchen.png" alt="">
              <h5>Yun-Chun (Johnny) Chen</u></a></h5>2018/07–2019/03
			  <br>Next Job: Short-term Visiting Scholar, Bradley Department of Electrical and Computer Engineering, Virginia Tech<br>
 <img src="images/hyperlink_icons.png" style="width: 11.5%;height: 11.5%;"><a class="readon" href="https://yunchunchen.github.io/" target="_blank" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
            </div></div>
			
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/yccheng.jpg" alt="">
              <h5>Yu-Chiang Cheng   </h5>2017/09–2019/02
			  <br>Next Job:
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ctchou.png" alt="">
              <h5>Chao-Te Chou</u></a></h5>2017/07–2019/02
			  <br>Next Job: Military service<br>
 <img src="images/hyperlink_icons.png" style="width: 11.5%;height: 11.5%;"><a class="readon" href="https://chao-te.github.io/" target="_blank" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
            </div></div>
            
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="img/team/ylliu.jpg" alt="">
              <h5>Yu-Lun Liu</u></a></h5>2017/09–2018/09
			  <br>Next Job: Ph.D. Student, Computer Science and Information Engineering, National Taiwan University<br>
<img src="images/hyperlink_icons.png" style="width: 11.5%;height: 11.5%;"> <a class="readon" href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/" target="_blank" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ymyeh.png" alt="">
              <h5>Yang-Ming Yeh   </h5>2015/09–2018/07
			  <br>Next Job: Ph.D. Student, Graduate Institute of Electronics Engineering, National Taiwan University
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/tkhu.jpg" alt="">
              <h5>Ting-Kuei Hu</u></a></h5>2015/09–2018/07
			  <br>Next Job: M.S. Student, Computer Science & Engineering, Texas A&M University<br>
 <img src="images/hyperlink_icons.png" style="width: 11.5%;height: 11.5%;"><a class="readon" href="https://www.linkedin.com/in/tkhu1992/" target="_blank" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ykchiu.jpg" alt="">
              <h5>Yu-Kai Chiu   </h5>2018/05–2018/07
			  <br>Next Job: M.S. Student, Master of Entertainment Technology, School of Computer Science, Carnegie Mellon University
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/lyyu.jpg" alt="">
              <h5>Li-Yu Yu   </h5>2016/07–2017/11
			  <br>Next Job: M.S. Student, Electrical Engineering, National Taiwan University
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/pxhuang.jpg" alt="">
              <h5>Po-Hsiang Huang   </h5>2016/07–2017/11
			  <br>Next Job: M.S. Student, Electrical Engineering, National Taiwan University
            </div></div>
			
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/fychuang.jpg" alt="">
              <h5>Fu-Yu Chuang   </h5>2016/07–2017/09
			  <br>Next Job: M.S. Student, Electrical Engineering, National Taiwan University
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/cyyang.jpg" alt="">
              <h5>Chu-Ya Yang   </h5>2016/07–2017/08
			  <br>Next Job: Research assistant, IIS, Academia Sinica -> M.S. Student, Computer Science, Texas A&M University
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/hhchang.jpg" alt="">
              <h5>Hao-Hsuan Chang</u></a></h5>2016/09–2017/06
			  <br>Next Job: Ph.D. Student, Electrical and Computer Engineering, Virginia Tech<br>
 <img src="images/hyperlink_icons.png" style="width: 11.5%;height: 11.5%;"><a class="readon" href="https://www.linkedin.com/in/hao-hsuan-chang-8862b7170/" target="_blank" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ananyu.jpg" alt="">
              <h5>An An Yu   </h5>2016/01–2016/07
			  <br>Next Job: Google
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/wrliu.jpg" alt="">
              <h5>Wan-Rou Liu   </h5>2016/01–2016/05
			  <br>
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/phhsiao.jpg" alt="">
              <h5>Pai-Heng Hsiao   </h5>2013/11–2015/12
			  <br>Next Job: Umbo CV -> Memorence AI
            </div></div>
			
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/wslai.jpg" alt="">
              <h5>Wei-Sheng Lai</u></a></h5>2014/09–2015/07
			  <br>Next Job: Ph.D. Student, Electrical Engineering and Computer Science, University of California, Merced<br>
 <img src="images/hyperlink_icons.png" style="width: 11.5%;height: 11.5%;"><a class="readon" href="https://www.wslai.net/" target="_blank" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/hhsu.jpg" alt="">
              <h5>Hsiao-Hang Su   </h5>2014/07–2015/04 
			  <br>Next Job: Undergraduate, School of Dentistry, Kaohsiung Medical University
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/mhchen.jpg" alt="">
              <h5>Min-Hung Chen</u></a></h5>2013/07–2014/07
			  <br>Next Job: Ph.D. Student, Electrical and Computer Engineering, Georgia Institute of Technology<br>
 <img src="images/hyperlink_icons.png" style="width: 11.5%;height: 11.5%;"><a class="readon" href="https://www.linkedin.com/in/chensteven/" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/htchen.jpg" alt="">
              <h5>Hsiao-Tung Chen</u></a></h5>2013/07–2014/07
			  <br>Next Job: M.S. Student, Electrical and Computer Engineering, Cornell University -> Amazon<br>
 <img src="images/hyperlink_icons.png" style="width: 11.5%;height: 11.5%;"><a class="readon" href="https://www.linkedin.com/in/hsiao-tung-chen-85748695/" target="_blank" style="hovercolor:#6aa4ab"">Website</a><br>
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/hychen.jpg" alt="">
              <h5>Hsin-Yi Chen</u></a></h5>2011/07–2014/06
			  <br>Next Job: Ph.D. Student, Computer Science and Information Engineering, National Taiwan University -> NVIDIA<br>
 <img src="images/hyperlink_icons.png" style="width: 11.5%;height: 11.5%;"><a class="readon" href="https://www.linkedin.com/in/hsin-i-chen-67177977" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/fjchang.jpg" alt="">
              <h5>Feng-Ju Chang</u></a></h5>2011/08–2013/07
			  <br>Next Job: Ph.D. Student, Electrical Engineering, University of Southern California<br>
 <img src="images/hyperlink_icons.png" style="width: 11.5%;height: 11.5%;"><a class="readon" href="https://www.linkedin.com/in/feng-ju-claire-chang-28329259/" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ycchang.jpg" alt="">
              <h5>Yao-Chuan Chang   </h5>2012/06–2013/06
			  <br>Next Job: Ph.D. Student, Biomedical Engineering, University of Southern California
            </div></div>
			
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/jttsai.jpg" alt="">
              <h5>Jeng-Tsung Tsai   </h5>2012/07–2013/06
			  <br>Next Job: M.S. Student, Computer Science, University of Southern California -> MathWorks
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/jhhua.jpg" alt="">
              <h5>Ju-Hsuan Hua</h5>2012/08–2013/06
			  <br>Next Job: M.S. Student, Robotics, Carnegie Mellon University
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/tylin.png" alt="">
              <h5>Tsung-Yi Lin</u></a></h5>2011/02–2011/07
			  <br>Next Job: Ph.D. Student, Computer Science, University of California, San Diego<br>
 <img src="images/hyperlink_icons.png" style="width: 11.5%;height: 11.5%;"><a class="readon" href="https://www.linkedin.com/in/tsung-yi-lin-48a4b541/" target="_blank" style="hovercolor:#6aa4ab">Website</a><br>
            </div></div>

        </div>         
       
        
    
     <!-- _______________ _______Alumni______________________-->                                                                 
      </div>
    </section>

	<!-- Publications Publications Publications Publications-->

    <section class="bg-light" id="publications">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading" >PUBLICATIONS</h2>
                    <!--<h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3>-->
                </div>
            </div>
            
			<iframe src="https://vl-lab.github.io/publications_icon.html" width="100%" height="11000px" frameborder="0" scrolling="yes"></iframe>

   		</div>
    </section>
	<!-- Publications Publications Publications Publications-->



    <!-- Contact -->
    <section id="contact">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
			  <h2 class="section-heading text-uppercase">Contact Us</h2>
          </div>
        </div>
        <div class="mainbody" style="color:whitesmoke;font-family: 'Montserrat', 'Helvetica Neue', Helvetica, Arial, sans-serif;margin-bottom: 2em">
        <h4 style="color: #fed136">Lab Location</h4>
        <ol>EC118, Engineering Building 3, 1001 University Road, Hsinchu 300, Taiwan</ol>
        <div class="col-sm-8 col-xs-12">
                                    <div class="locations_map">
                                        <iframe
                                            width="700"
                                            height="400"
                                            frameborder="0" style="border:2px-solid-blue;"
                                            src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d3622.2468276278687!2d120.99525951539745!3d24.787000254301407!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x3468360f95ae32a7%3A0xdc6638191f31d7a9!2z5Lqk6YCa5aSn5a245bel56iL5LiJ6aSo!5e0!3m2!1szh-TW!2stw!4v1568094073296!5m2!1szh-TW!2stw" allowfullscreen>
                                            </iframe>
                                    </div>
                                </div>
        </div>

      </div>
    </section>

    <!-- Recruitment -->
    <section id="recruit">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
         <iframe src="recruitment.html?v20191121" width="80%" height="1700" frameborder="0" scrolling="yes"></iframe> 
          </div>
        </div>
        <br>
      </div>
    </section>

    <!-- Footer -->
    <footer>
      <div class="container">
        <div>
       <span class="copyright">2019 © Vision and Learning Lab, <a href="https://www.cs.nctu.edu.tw/" target="_blank">National Chiao Tung University</a>. All Rights Reserved. | <a href="https://drive.google.com/file/d/1G7flFb3xFrVo1V7EZ5quLNtfRMhQ-sg3/view?usp=sharing" target="_blank">Safety Policy</a> & <a href="http://secretariat.nctu.edu.tw/tw/intellectual/" target="_blank">Privacy Policy</a></span>
        </div>
      </div>
    </footer>
  
    <!-- Oasis -->
    <div class="portfolio-modal modal fade" id="ActionRecognition" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>Action Recognition</h3>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td>

<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><span style="font-size: medium; font-weight: bold; color: #000000;"> We aim to resolve the difficulties of action recognition arising from the large intra-class variations. These unfavorable variations make it infeasible to represent one action instance by other ones of the same action. We hence propose to extract both </span> <span style="color: #ff9900;"> <span style="font-size: medium; font-weight: bold; font-style: italic;"> instance-specific</span></span><span style="font-size: medium; font-weight: bold; color: #000000;"> and </span> <span style="color: #ff9900;"> <span style="font-size: medium; font-weight: bold; font-style: italic;"> class-consistent</span></span><span style="font-size: medium; font-weight: bold; color: #000000;"> features to facilitate action recognition. Specifically, the instance-specific features explore the self-similarities among frames of each video instance, while class-consistent features summarize withinclass similarities. We introduce a generative formulation to combine the two diverse types of features. The experimental results demonstrate the effectiveness of our approach.</span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">　</p>
<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Instance-specific and Class-consistent Cues</span></strong></p>
<ul style="text-align: justify;">
<li><span style="font-size: medium; color: #000000;">We aim to resolve the difficulties of action recognition arising from the <em>large intra-class variations</em>. These unfavorable variations make it infeasible to represent one action instance by other ones of the same action. We hence propose to extract both <em>instance-specific</em> and <em>class-consistent</em> features to facilitate action recognition.</span></li>
<li><span style="font-size: medium; color: #000000;"><strong>Credits</strong></span>
<ul style="text-align: justify;">
<li><span style="font-size: medium; color: #000000;"><strong>Instance-specific features: </strong>Self-similarities among frames of an action sequence. <em>Multivariate linear prediction (MLP)</em> is adopted to aggregate all the causalities among frames. </span></li>
<li><span style="font-size: medium; color: #000000;"><strong>Class-consistent features: </strong>Characteristics shared by instances of the same action. <em>Support vector machines (SVMs)</em> are used to discover these features based on the bag-of-words model. </span></li>
<li><span style="font-size: medium; color: #000000;">We propose a <em>generative</em> formulation to integrate the two complementary types of features, and boost the performance. </span></li>
</ul>
</li>
</ul>

<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Core Techniques</span></strong></p>
<ul style="text-align: justify;">
<li><span style="font-size: medium; color: #000000;">We view actions as multivariate time signals. For signal processing in our approach, there are several essential techniques:</span>
<ul style="text-align: justify;">
<li><span style="font-size: medium; color: #000000;">Wide-Sense Stationary Process</span></li>
<li><span style="font-size: medium; color: #000000;">Linear Prediction</span></li>
<li><span style="font-size: medium; color: #000000;">Support Vector Machine</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;">We propose a generative model. The main idea is to consider the static and dynamic information of a multivariate time signal separately. This is based on the assumption that these two information are independent for action recognition.</span>
<ul style="text-align: justify;">
<li><span style="font-size: medium; color: #000000;">Instance-specific cue via MLP</span></li>
<li><span style="font-size: medium; color: #000000;">Class-consistent cue via SVM</span></li>
<li><span style="font-size: medium; color: #000000;">Linear fusion</span><br /> 　</li>
</ul>
</li>
</ul>
<p><span style="font-size: medium;"><img src="images/research/ar/ar_2.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>

<table border="0" style="text-align: left">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/icip-lin12.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6>Action Recognition using Instance-specific and Class-consistent Cues</h6>
Chin-An Lin, Yen-Yu Lin, Hong-Yuan Mark Liao, and Shyh-Kang Jeng<br>
<em>IEEE International Conference on Image Processing (ICIP), September 2012</em><br>
<a class="readon" href="images/paper/icip-lin12.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 	

                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- HR -->
    <div class="portfolio-modal modal fade" id="MultimediaRetrieval" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>Content-based Multimedia Retrieval</h3>
				  
						
<table style="border-collapse: collapse; width: 100%; text-align: justify;" border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td>
<p align="center"><img src="images/research/cbmr/cbmr_1.png" border="0" alt="" /></p>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><span style="font-size: medium; font-weight: bold; color: #000000;">The development of image querying using content-based image retrieval (CBIR) techniques has attracted a great attention owing to its abundant applicability. We are particularly interested in how the user's semantics could be integrated into a CBIR system. The user's semantics for CBIR involves two different sources of information: the similarity relations entailed by the content-based features, and the </span><span style="color: #ff9900;"> <span style="font-size: medium; font-weight: bold; font-style: italic;">relevance relations</span></span><span style="font-size: medium; font-weight: bold; color: #000000;"> specified in the feedback. Besides, we also look into the issues of selecting a good feature set for improving the retrieval performance. </span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">　</p>
<p><span style="font-size: x-large;"><strong><span style="color: #ffcc00;"> Semantic Manifold Learning for Image Retrieval</span></strong></span></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>To address the problem of CBIR with relevance feedback by manifold learning</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Two aspects of information are being fused: <em>Intrinsic Similarity Relations</em> and <em>Query &amp; Relevance Feedback</em></span></li>
<li><span style="font-size: medium; color: #000000;">User-specific Semantic Manifold</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>A manifold-learning technique to effectively fuse the multi-modality of information </strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;"><em>Intrinsic Similarity Relations</em>: abundant, and auxiliary</span></li>
<li><span style="font-size: medium; color: #000000;"><em>User Relevance Feedback</em>: few, but crucial</span></li>
</ul>
</li>
</ul>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>User relevance feedback serves as augmented relations to intrinsic similarity relations</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Augmented Relations Embedding (ARE)</span></li>
<li><span style="font-size: medium; color: #000000;">Augmented features: global and local image features</span></li>
</ul>
</li>
</ul>
<p align="center"><img src="images/research/cbmr/cbmr_2.png" border="0" alt="" /></p>

<p><strong><span style="font-size: x-large; color: #ffcc00;">Features</span></strong></p>
<ul>
<li><span style="color: #000000; font-size: medium;"><strong>Global</strong></span>
<ul>
<li><strong><span style="color: #000000; font-size: medium;">Color</span></strong>
<ul>
<li><span style="font-size: medium; color: #000000;">Quantized HSV color histogram: 64 dimensions</span></li>
<li><span style="font-size: medium; color: #000000;">First three moments in each color channel: 9 dimensions</span></li>
<li><span style="font-size: medium; color: #000000;">Color coherence vector: 128 dimensions</span></li>
</ul>
</li>
<li><strong><span style="color: #000000; font-size: medium;">Texture</span></strong>
<ul>
<li><span style="font-size: medium; color: #000000;">Tamura coarseness: 10 dimensions</span></li>
<li><span style="font-size: medium; color: #000000;">Tamura directionality: 8 dimensions</span></li>
</ul>
</li>
<li><strong><span style="color: #000000; font-size: medium;">Wavelet</span></strong>
<ul>
<li><span style="color: #000000; font-size: medium;">3-level DWT image decomposition</span></li>
<li><span style="color: #000000; font-size: medium;">The first two moments in each high-frequency sub-bands: 18 dimensions</span></li>
</ul>
</li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Local</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Detecting salient regions (associated with interest points)</span></li>
<li><span style="font-size: medium; color: #000000;">Describing the local properties of each salient region</span></li>
<li><span style="font-size: medium; color: #000000;">Difference-of-Gaussian (DoG) for salient-region detection [Lowe 2004]</span></li>
</ul>
</li>
</ul>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>Augmented Features</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">The proposed image representation</span></li>
<li><span style="font-size: medium; color: #000000;">Augmenting the local-feature scheme with global image properties so that the vector quantization just described can be used to derive a <em>k</em>-dimensional vector</span></li>
</ul>
</li>
</ul>
<p align="center"><img src="images/research/cbmr/cbmr_3.png" border="0" alt="" /></p>

<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Visualization of Embedding Space</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>The iterative procedure</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Use ARE to compute a 30-D semantic manifold</span></li>
<li><span style="font-size: medium; color: #000000;">Project data points onto a 2-D plane by multidimensional scaling (MDS) [Cox and Cox, 1994] for visualization</span></li>
</ul>
</li>
</ul>
<table style="width: 100%;" border="1">
<tbody>
<tr>
<td align="center"><span style="color: #0000ff;"><img src="images/research/cbmr/cbmr_4.png" border="0" alt="" width="150" /></span>
<p align="center"><span style="color: #0000ff;"><strong><span style="font-size: medium;">Firework</span></strong></span></p>
</td>
<td>
<p align="center"><img src="images/research/cbmr/cbmr_6.gif" border="0" width="700" height="438" /></p>
</td>
</tr>
<tr>
<td align="center"><span style="color: #0000ff;"><img src="images/research/cbmr/cbmr_5.png" border="0" alt="" width="150" /></span>
<p align="center"><span style="color: #0000ff;"><strong><span style="font-size: medium;">Office Interiors</span></strong></span></p>
</td>
<td>　<img src="images/research/cbmr/cbmr_7.gif" border="0" width="700" height="438" style="display: block; margin-left: auto; margin-right: auto;" /></td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>　</p>
<p style="text-align: left"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>

<table border="0" style="text-align: left">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/mm-lin05.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6><span style="line-height: 1.3em;">Semantic Manifold Learning for Image Retrieval</span></h6>
<span style="line-height: 1.3em;">Yen-Yu Lin, Tyng-Luh Liu, and Hwann-Tzong Chen</span><br />
<span style="line-height: 1.3em;"><em>ACM International Conference on Multimedia (ACM MM), December 2005, <span style="color: #00ccff;">(Best Student Papers Session)</span></em></span>
<br />
<a class="readon" href="images/paper/mm-lin05.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 
                                             
                   	 </tbody>
                  </table>

<!--
                    [<a href="" target="_blank">Conference</a>] 
                    [<a href="" target="_blank">Demo</a>]
-->
                    </li>
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
   

    <!-- Modal 1 -->
    <div class="portfolio-modal modal fade" id="Classification" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>Data Classification and Clustering</h3>

<table style="border-collapse: collapse; width: 100%; text-align: justify;" border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td><img src="images/research/dcc/dcc_1.png" border="0" alt="" width="800" style="display: block; margin-left: auto; margin-right: auto;" />
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><span style="font-size: medium; font-weight: bold; color: #000000;">Data, including observations, measurements or images are quantized/characterized with certain feature representations in the digital world for further processing. However, there exists no a universal way to well-depict all the instances. Particularly, the optimal data descriptors often vary from class to class. We are thus motivated to fuse <span style="color: #ff9900;"><em>multiple kernel learning</em> (MKL)</span> into the training procedure, and carry out a class-specific feature selection framework, which significantly facilitates the relevant tasks, such as clustering and classification.</span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">　</p>
<p><span style="font-size: x-large;"><strong><span style="color: #ffcc00;"> Multiple Kernel Learning with Local Learning</span></strong></span></p>
<ul>
<li><span style="font-size: medium; color: #000000;">We address two unfavorable issues of local learning, i.e., high risk of overfitting and heavy computational cost, and present an efficient boosting algorithm to learn sample-specific local classifiers for object category recognition.</span></li>
</ul>
<ul>
<li><span style="color: #000000;"><strong><span style="font-size: medium;">Our approach</span></strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">We cast the multiple, independent training processes of local classifiers as a correlative multi-task learning problem.</span></li>
<li><span style="font-size: medium; color: #000000;">We establish a parametric space where these local classifiers lie and spread as a manifold-like structure.</span></li>
<li><span style="font-size: medium; color: #000000;">By designing a new multi-task boosting algorithm, the local classifiers are obtained by completing the manifold embedding.</span></li>
<li><span style="font-size: medium; color: #000000;">The algorithm carries out incremental multiple kernel learning.</span></li>
</ul>
<p><img src="images/research/dcc/dcc_2.png" border="0" alt="" width="800" style="display: block; margin-left: auto; margin-right: auto;" /></p>
</li>
</ul>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Cluster-dependent Feature Selection</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;">A <strong><em>chicken-and-egg</em></strong> problem.</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Clustering VS. Feature selection: <em><strong>Is it the clustering that contributes to the feature selection, or the feature selection that boosts the clustering?</strong></em></span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;">Again, the optimal data descriptors often vary from cluster to cluster.</span></li>
<li><span style="font-size: medium; color: #000000;">With the idea of associating each cluster with a learnable ensemble kernel, we integrate multiple kernel learning into the clustering procedure, and cast it as a joint optimization problem.</span></li>
</ul>
<p><img src="images/research/dcc/dcc_3.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
</td>
</tr>
</tbody>
</table>

<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>

<table border="0" cellspacing="0" cellpadding="0"  style="text-align: justify;">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/icpr-huang12.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6>Cluster-dependent Feature Selection by Multiple Kernel Self-organizing Map</h6>
Kuan-Chieh Huang, Yen-Yu Lin, and Jie-Zhi Cheng
<br />
<em>IEEE International Conference on Pattern Recognition (ICPR), November 2012</em>
<br />
<a class="readon" href="images/paper/icpr-huang12.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
<tr>
<td valign="top"><img src="images/thumbs/eccv-lin10.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6><span style="line-height: 1.3em;">Clustering Complex Data with Group-dependent Feature Selection</span></h6>
<span style="line-height: 1.3em;">Yen-Yu Lin, Tyng-Luh Liu, and Chiou-Shann Fuh</span>
<br />
<span style="line-height: 1.3em;"> <em>European Conference on Computer Vision (ECCV), September 2010</em></span>
<br>
<a class="readon" href="images/paper/eccv-lin10.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
<tr>
<td valign="top"><img src="images/thumbs/iccv-lin09.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6><span style="line-height: 1.3em;">Efficient Discriminative Local Learning for Object Recognition</span></h6>
<span style="line-height: 1.3em;">Yen-Yu Lin, Jyun-Fan Tsai, and Tyng-Luh Liu</span>
<br />
<span style="line-height: 1.3em;"> <em>IEEE International conference on Computer Vision (ICCV), September 2009</em></span>
<br />
<a class="readon" href="images/paper/iccv-lin09.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 										

                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 2 -->
    <div class="portfolio-modal modal fade" id="FaceDetection" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>Face Detection</h3>


<table style="border-collapse: collapse; width: 100%; text-align: justify;" border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td><img src="images/research/fd/fd_1.png" border="0" alt="" width="800" style="display: block; margin-left: auto; margin-right: auto;" />
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><strong><span style="color: #000000; font-size: medium;">We aim to design a general learning framework for face detection while handling some problems caused by a variety of variations in images, including <span style="color: #ff9900;"><em>Profile, Rotation, Occlusion, Lighting Conditions, Varied Expressions, Multiple Faces</em> and <em>Scales</em></span>. We are motivated to formulate the task as a classification problem over data of multiple classes. Our approach takes advantage of a multi-class boosting algorithm, MBHboost, to effectively perform face detection with the assistance of its integration with a cascade structure. As a result, it features great flexibility in the sense that only one single boosted cascade is needed without worrying about how to select the most appropriate cascade for the detection.</span></strong></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">　</p>
<p><span style="font-size: x-large;"><strong><span style="color: #ffcc00;">Real-time, Multi-view Face Detection</span></strong></span></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong><span style="line-height: 1.3em;">A real-time, multi-view face detector</span></strong></span></li>
<ul>
<li><span style="line-height: 1.3em; font-size: medium; color: #000000;">Multi-view: Profile faces, rotated faces, faces with partial occlusions, or faces under different lighting conditions</span></li>
<li><span style="line-height: 1.3em; font-size: medium; color: #000000;">Real-time: At least <span style="font-weight: bold; color: #ff9900;">15</span> frames per second on a PC</span></li>
</ul>
<li><span style="font-size: medium; color: #000000;"><strong><span style="line-height: 1.3em;">Two key components</span></strong></span></li>
<ul>
<li><span style="line-height: 1.3em; font-size: medium; color: #000000;">A new boosting algorithm: Multi-class Bhattacharyya boost (MBHBoost)</span></li>
<li><span style="line-height: 1.3em; font-size: medium; color: #000000;">A new detection architecture: Multi-class cascade</span></li>
</ul>
<li><span style="line-height: 1.3em; font-size: medium; color: #000000;">The detector leads to a computational cost sub-linear to the number of face classes (views)</span></li>
</ul>
<p><span style="color: #ffcc00;"> </span></p>
<p><strong><span style="color: #ffcc00; font-size: x-large;">Key Features / Techniques</span></strong></p>
<ul>
<li><span style="line-height: 1.3em; font-size: medium; color: #000000;"> <span style="font-weight: bold;">Vector-valued</span> weak learners</span></li>
<li><span style="line-height: 1.3em; font-size: medium; color: #000000;">A new boosting algorithm: Multi-class Bhattacharyya boost (MBHBoost)</span></li>
<li><span style="line-height: 1.3em; font-size: medium; color: #000000;">A new detection architecture: Multi-class cascade</span></li>
</ul>
<p><span style="font-size: x-large; color: #ffcc00;"><strong>Rectangle Features</strong></span></p>
<p><img src="images/research/fd/fd_2.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><span style="color: #ffcc00; font-size: x-large;"><strong> </strong></span></p>
<p><span style="color: #ffcc00; font-size: x-large;"><strong>Problems Caused by Thresholding and Our Solution</strong></span></p>
<p><img src="images/research/fd/fd_3.png" border="0" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><img src="images/research/fd/fd_4.png" border="0" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Classifier Sharing</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;">An vector-valued weak learner associated with is defined by </span><br /><span style="font-size: medium; color: #000000;"> <img src="images/research/fd/fd_5.png" border="0" style="display: block; margin-left: auto; margin-right: auto;" /></span></li>
<li><span style="color: #000000; font-size: medium;"><strong>Advantages</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Shared by all classes, no human knowledge or searching</span></li>
<li><span style="font-size: medium; color: #000000;">Each component independently learns a decision boundary</span></li>
<li><span style="font-size: medium; color: #000000;">Computational efficiency: The value of <em>k</em> is identical in all components</span></li>
</ul>
</li>
</ul>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Multi-class Cascade</span></strong></p>
<p><img src="images/research/fd/fd_6.png" border="0" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<ul>
<li><span style="font-size: medium; color: #000000;">Reduce the detection problem to a series of pattern rejection problem</span></li>
<li><span style="font-size: medium; color: #000000;">Speed up the detection process: Coarse-to-fine detection</span></li>
<li><span style="font-size: medium; color: #000000;">Deal with vector-valued outputs: Message-passing between stages</span></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>

<table border="0" cellspacing="0" cellpadding="10"  style="text-align: justify;">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/cvpr-lin05.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6><span style="line-height: 1.3em;">Robust Face Detection with Multi-class Boosting</span></h6>
<span style="line-height: 1.3em;">Yen-Yu Lin and Tyng-Luh Liu</span>
<br />
<span style="line-height: 1.3em;"> <em>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2005</span>
<br />
<a class="readon" href="images/paper/cvpr-lin05.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
<tr>
<td valign="top"><img src="images/thumbs/eccv-lin04.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6><span style="line-height: 1.3em;">Fast Object Detection with Occlusions</span></h6>
<span style="line-height: 1.3em;">Yen-Yu Lin, Tyng-Luh Liu, and Chiou-Shann Fuh</span>
<br />
<span style="line-height: 1.3em;"> <em>European Conference on Computer Vision (ECCV), May 2004</em></span>
<br />
<a class="readon" href="images/paper/eccv-lin04.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 


                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 3 -->
    <div class="portfolio-modal modal fade" id="FeatureMatching" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>Feature Matching</h3>
				  
<table style="border-collapse: collapse; width: 100%; text-align: justify;" border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td><img src="images/research/fm/fm_1.png" border="0" alt="" width="800" style="display: block; margin-left: auto; margin-right: auto;" />
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><span style="font-size: medium; font-weight: bold; color: #000000;">Feature matching, or feature correspondence serves as a core technique for image analysis and understanding. There is a wide range of applications that are closely related to it, such as object recognition, image retrieval, 3D reconstruction, image enhancement, and so on. The problems of correspondence involves clutter background, significant amount of outliers and occlusion. Moreoever, multiple translations, orientations and deformations also negatively affect the matching of features in terms of precision, recall and efficiency. To this end, we look into these problems and propose robust frameworks to resolve them.</span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">　</p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">What is the problem in feature matching?</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;">Motivation: For image matching, the initial feature correspondence set</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Can not be too large: <strong>Low recall </strong></span></li>
<li><span style="font-size: medium; color: #000000;">Contains corrupt matches: <strong>Low precision</strong></span></li>
<li><span style="font-size: medium; color: #000000;">Requires geometric checking: <strong>Time consuming</strong></span></li>
</ul>
</li>
</ul>
<p><span style="font-size: medium; color: #000000;"><img src="images/research/fm/fm_2.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<ul>
<li><span style="font-size: medium; color: #000000;">Proposed method: Alternate Hough and inverted Hough voting</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Correspondence mutual checking in homography space</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;">Main advantages: </span>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>High precision:</strong> Hough voting</span></li>
<li><span style="font-size: medium; color: #000000;"><strong>High recall:</strong> Inverted Hough voting </span></li>
<li><span style="font-size: medium; color: #000000;">Low computational cost: BPLR for voter filtering</span></li>
</ul>
</li>
</ul>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Core Ideas</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>Precision:</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">The correct matches are biased to a dense cluster in the transformation space</span></li>
<li><span style="font-size: medium; color: #000000;">We cast the task of feature matching problem into a density estimation problem</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Recall:</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Grouped features with high probability undergo similar transformations in matching</span></li>
<li><span style="font-size: medium; color: #000000;">We utilize the nature of BPLR to locate non-crossboundary regions which correspond to groups of similar transformations</span><br /> 　</li>
</ul>
</li>
</ul>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Core </span> <span style="font-size: x-large; color: #ffcc00;">Techniques</span></strong></p>
<ul>
<li><strong><span style="color: #000000; font-size: medium;">Hough voting</span></strong>
<ul>
<li><span style="font-size: medium; color: #000000;">The tentative correspondences are found via nearest-neighbor search in descriptor space and use to generate votes in the transformation space.</span></li>
<li><span style="font-size: medium; color: #000000;">We use density of each correspondence in the transformation space to verify its correctness</span></li>
</ul>
</li>
<li><strong><span style="color: #000000; font-size: medium;">Inverted hough voting</span></strong>
<ul>
<li><span style="color: #000000; font-size: medium;">Recommend each feature additional transformations by investigating density distribution of nearby features covered by the same BPLR.</span></li>
</ul>
</li>
</ul>
<p><span style="font-size: medium;"> <img src="images/research/fm/fm_3.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>
<table border="0" cellspacing="0" cellpadding="10" style="text-align: justify;">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/cvpr-chen13.png" border="0" alt="" width="170" height="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6>Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</h6>
Hsin-Yi Chen, Yen-Yu Lin and Bing-Yu Chen
<br />
<em>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), June 2013</em>
<br />
<a class="readon" href="images/paper/cvpr-chen13.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 			
				  
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Augmented Sensing -->
    <div class="portfolio-modal modal fade" id="ObjectRecognition" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>Object Recognition</h3>
				  
<table style="border-collapse: collapse; width: 100%; text-align: justify; " border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td><img src="images/research/or/or_1.png" border="0" alt="" width="800" style="display: block; margin-left: auto; margin-right: auto;" />
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><span style="font-size: medium; font-weight: bold; color: #000000;">Object recognition is one of the vision applications that deal with data of multiple classes. In such a case, not only discriminative features but also the class-specific features should be considered because the goodness of a feature representation for recognition is often <em><span style="color: #ff9900;">category-dependent</span></em>, and can even be object-dependent for the case of large intra-class variation. Thus, we aim to improve recognition accuracy by focusing on feature representation in terms of image features and similarity measures, where various feature representations in the domain of kernel matrices are fused to alleviate the difficulties caused by diverse forms of them.</span></p>
</td>
</tr>
</tbody>
</table>
<p><span style="font-size: x-large;"><strong><span style="color: #ffcc00;"> Object Recognition with Kernel Machines </span></strong></span></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>The problems and observations</strong></span>
<ul>
<li><span style="font-size: medium; background-color: #ffffff; color: #000000;">Diverse object categories</span></li>
<li><span style="font-size: medium; background-color: #ffffff; color: #000000;">Large intra-class variations</span></li>
<li><span style="font-size: medium; background-color: #ffffff; color: #000000;">The goodness of a feature representation for recognition is often category-dependent, and can even be object-dependent for the case of large intra-class variation</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Improve recognition accuracy by focusing on</strong></span>
<ul>
<li><span style="font-size: medium; background-color: #ffffff; color: #000000;">Image features</span></li>
<li><span style="font-size: medium; background-color: #ffffff; color: #000000;">Similarity measures</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Approach</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Fusing various feature representations in the domain of kernel matrices</span></li>
<li><span style="font-size: medium; color: #000000;">Learning a local ensemble kernel machine (e.g. local ensemble kernel + SVM) for each training sample</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Localized kernel alignment (initialization)</span></li>
<li><span style="font-size: medium; color: #000000;">MRF modeling and optimization</span></li>
<li><span style="font-size: medium; color: #000000;">Associate each sample with a SVM classifier</span></li>
</ul>
</li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Feature Fusion and Kernel Alignment</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Fusing feature representations is now combining kernels</span></li>
<li><span style="font-size: medium; color: #000000;">The effectiveness of a possible fusion can therefore be reasonably estimated by how good an ensemble kernel is</span></li>
<li><span style="font-size: medium; color: #000000;">We consider target kernel alignment for measuring the goodness of a kernel. [Cristianini et al. '01]</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Optimization over a MRF Model</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Retain the effectiveness of each local classifier</span></li>
<li><span style="font-size: medium; color: #000000;">Alleviate the possible overfitting</span></li>
<li><span style="font-size: medium; color: #000000;">Reduce the redundancy of the local classifiers</span></li>
</ul>
</li>
</ul>
<p><img src="images/research/or/or_2.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong style="font-size: 10px; line-height: 1.3em;"><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong style="font-size: 10px; line-height: 1.3em;"><span style="font-size: x-large; color: #ffcc00;">Multiple Kernel Learning (MKL) for Dimensionality Reduction</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>Observations</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">No single feature representation suffices to explain the complexity of the whole data</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Goals</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Perform MKL for heterogeneous feature fusion</span></li>
<li><span style="font-size: medium; color: #000000;">Generalize a set of dimensionality reduction methods to consider multiple kernels</span></li>
<li><span style="font-size: medium; color: #000000;">Extend MKL from supervised learning to unsupervised and semi-supervised learning</span></li>
<li><span style="font-size: medium; color: #000000;">Improve performances of vision applications by using multiple feature representations</span><br /> 　</li>
</ul>
</li>
</ul>
<p><img src="images/research/or/or_3.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>
<table border="0" cellspacing="0" cellpadding="10" style="text-align: justify;">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/pami-lin11.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6>Multiple Kernel Learning for Dimensionality Reduction</h6>
Yen-Yu Lin, Tyng-Luh Liu, and Chiou-Shann Fuh
<br />
<em>IEEE Transction on Pattern Analysis and Machine Intelligence (TPAMI), Vol. 33, No. 6, June 2011</em>
<br />
<a class="readon" href="images/paper/pami-lin11.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
<tr>
<td valign="top"><img src="images/thumbs/nips-lin08.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6><span style="line-height: 1.3em;">Dimensionality Reduction for Data in Multiple Feature Representations</span></h6>
<span style="line-height: 1.3em;">Yen-Yu Lin, Tyng-Luh Liu, and Chiou-Shann Fuh</span>
<br />
<span style="line-height: 1.3em;"> <em>Advances in Neural Information Processing Systems (NIPS), December 2008</em></span>
<br />
<a class="readon" href="images/paper/nips-lin08.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
<tr>
<td valign="top"><img src="images/thumbs/cvpr-lin07.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6><span style="line-height: 1.3em;">Local Ensemble Kernel Learning for Object Category Recognition</span></h6>
<span style="line-height: 1.3em;">Yen-Yu Lin, Tyng-Luh Liu, and Chiou-Shann Fuh</span>
<br />
<span style="line-height: 1.3em;"> <em>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), June 2007</em></span>
<br />
<a class="readon" href="images/paper/cvpr-lin07.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 			
				  
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>   
  
       <!-- SIMTY Wakeup Management -->
    <div class="portfolio-modal modal fade" id="PeopleCounting" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>People Counting</h3>
				  
<table style="border-collapse: collapse; width: 100%; text-align: justify;" border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td><img src="images/research/pc/pc_1.png" border="0" alt="" width="800" style="display: block; margin-left: auto; margin-right: auto;" />
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><span style="font-size: medium; font-weight: bold; color: #000000;">The goal of people counting is to estimate the number of people or the density of crowds in a monitored environment. Both the long-term and short-term statistics of people counts of an environment provide useful information for strategy planning or event detection. However, detecting or estimating the density of crowds is always a challenging task due to some potential difficulties, such as <span style="color: #ff9900;"><em>partial occlusions, low-quality images, clutter backgrounds</em></span>, and so on. To this end, we focus on the framework where multiple cameras with different angles of view are available, and consider the visual cues captured by each camera as a knowledge source, carrying out cross-camera knowledge transfer to alleviate the difficulties.</span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">　</p>
<p><span style="font-size: x-large;"><strong><span style="color: #ffcc00;"> Single-Camera People Counting</span></strong></span></p>
<ul>
<li><span style="color: #000000; font-size: medium;"><strong>Cross-camera people counting</strong></span>
<ul>
<li><span style="color: #000000; font-size: medium;">Applicable to various environments</span></li>
<li><span style="color: #000000; font-size: medium;">Online training data acquisition and camera perspective estimation</span></li>
</ul>
</li>
</ul>
<ul>
<li><span style="color: #000000; font-size: medium;"><strong>Occlusion handling: Coupled Gaussian processes </strong></span>
<ul>
<li><span style="color: #000000; font-size: medium;">First-pass Gaussian processes: Visible part</span></li>
<li><span style="color: #000000; font-size: medium;">Second-pass Gaussian processes: Occluded part</span></li>
</ul>
</li>
</ul>
<p><img src="images/research/pc/pc_2.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Why Predict Conflict Works</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;">An example: prediction conflict between horizontal and vertical gradients for occlusion handling</span></li>
</ul>
<p><span style="font-size: medium;"><img src="images/research/pc/pc_3.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<ul>
<ul>
<li><span style="color: #000000; font-size: medium;"><strong>Legend</strong></span>
<ul>
<li><span style="font-size: medium;"><strong><span style="color: #ff0000;">---- </span></strong> <span style="color: #000000;">Ground truth</span></span></li>
<li><span style="font-size: medium;"><span style="color: #0000ff;"><strong>----</strong> </span> <span style="color: #000000;">Prediction by the feature of</span> <span style="color: #0000ff;"><strong>horizontal</strong></span> <span style="color: #000000;">gradients</span></span></li>
<li><span style="font-size: medium;"><span style="color: #00ff00;"><strong>----</strong> </span> <span style="color: #000000;">Prediction by the feature of</span> <span style="color: #00ff00;"><strong>vertical</strong></span><span style="color: #000000;"> gradients</span></span></li>
</ul>
</li>
</ul>
</ul>
<p> </p>
<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Blob Representation</span></strong></p>
<ul>
<li><span style="color: #000000; font-size: medium;">We focus on foreground objects (pedestrians) in images</span></li>
<li><span style="color: #000000; font-size: medium;">Background subtraction</span></li>
<li><span style="color: #000000; font-size: medium;">Grouping spatially connected pixels</span></li>
</ul>
<p><img src="images/research/pc/pc_4.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p> </p>
<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">People Counting with Multiple Cameras</span></strong></p>
<ul>
<li><span style="color: #000000; font-size: medium;"><strong>Why multiple cameras?</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Complementary information</span></li>
<li><span style="font-size: medium; color: #000000;">Dealing with resolution issues, occlusions, ...</span></li>
</ul>
</li>
</ul>
<ul>
<li><span style="color: #000000; font-size: medium;"><strong>Our approach</strong></span>
<ul>
<li><span style="color: #000000; font-size: medium;">Ground plane matching + Visual knowledge transfer</span></li>
</ul>
</li>
</ul>
<p><img src="images/research/pc/pc_5.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p> </p>
<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Why Bob Matching?</span></strong></p>
<ul>
<li><span style="color: #000000; font-size: medium;">Find the same groups of pedestrians across cameras</span>
<ul>
<li><span style="font-size: medium;"><span style="color: #000000;"><strong>Synchronized frames</strong></span></span>
<ul>
<li><span style="font-size: medium; color: #000000;">The numbers of people are not always equal, particularly when FOVs are quite different</span></li>
</ul>
</li>
<li><span style="color: #000000;"><span style="font-size: medium; line-height: 1.3em;">We work on <strong>corresponding blob sets</strong>:</span><span style="font-size: medium; line-height: 1.3em;"> </span><strong style="font-size: medium; line-height: 1.3em;">blob clusters</strong></span></li>
</ul>
</li>
</ul>
<p><img src="images/research/pc/pc_6.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p> </p>
<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Observation</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;">Approximating the people counts in an image in two parts</span>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>Regular part:</strong> intra-camera visual features</span></li>
<li><span style="font-size: medium; color: #000000;"><strong>Residual part:</strong> inter-camera visual knowledge</span></li>
</ul>
</li>
<li>
<p><span style="color: #000000; font-size: medium;">Formulate it as a transfer learning problem</span></p>
</li>
</ul>
<p><img src="images/research/pc/pc_7.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>
<table border="0" cellspacing="0" cellpadding="10" style="text-align: justify;">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/mm-weng12.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6>Visual Knowledge Transfer among Multiple Cameras for People Counting with Occlusion Handling</h6>
<span>Ming-Fang Weng, Yen-Yu Lin, Nick C. Tang, and Hong-Yuan Mark Liao</span>
<br />
<em>ACM International Conference on Multimedia (MM), October 2012,<span style="color: #00ccff;"> (full paper)</span></em>
<br />
<a class="readon" href="images/paper/mm-weng12.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
<tr>
<td valign="top"><img src="images/thumbs/wifs-lin11.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6><span style="line-height: 1.3em;">Cross Camera People Counting with Perspective Estimation and Occlusion Handling</span></h6>
<span style="line-height: 1.3em;">Tsung-Yi Lin, Yen-Yu Lin, Ming-Fang Weng, Yu-Chiang Wang, Yu-Feng Hsu, and Hong-Yuan Mark Liao</span>
<br />
<span style="line-height: 1.3em;"> <em>IEEE International Workshop on Information Forensics and Security (WIFS), November 2011</em></span>
<br />
<a class="readon" href="images/paper/wifs-lin11.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 			
				  
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  
        <!-- ucsg -->
    <div class="portfolio-modal modal fade" id="SemanticSegmentation" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>Semantic Segmentation</h3>
				  
<table style="border-collapse: collapse; width: 100%; text-align: justify;" border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td><img src="images/research/ss/ss_1.png" border="0" alt="" width="800" style="display: block; margin-left: auto; margin-right: auto;" />
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><span style="font-size: medium; font-weight: bold; color: #000000;"> We focus on semantic segmentation, where class-based image segmentation is of focus as the task of labeling pixels with several pre-defined object classes or background in an image. Distinct from the image driven segmentation task, class based image segmentation aims to not only identify the object classes of interest, but also determine the shapes or boundaries of these objects. It in fact involves resolving two of the most fundamental problems in vision research: </span> <span style="color: #ff9900;"><span style="font-size: medium; font-weight: bold;"> <em>recognition</em></span></span><span style="font-size: medium; font-weight: bold; color: #000000;"> and </span><span style="color: #ff9900;"> <span style="font-size: medium; font-weight: bold;"> <em>segmentation</em></span></span><span style="font-size: medium; font-weight: bold; color: #000000;">. Therefore, it plays an essential role in many high-level computer vision applications, such as image and scene understanding.</span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">　</p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Class-based Image Segmentation</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>A concise annotation method for collecting training data for class based image segmentation. Two steps</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Generate multiple tight segments, combining the multiple segment method with the concept of bounding box prior</span></li>
<li><span style="font-size: medium; color: #000000;">Select the best segment by semi-supervised regression</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Credits</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Present a novel algorithm which integrates the bounding box prior into the concept of multiple image segmentation, and automatically generate multiple tight segments</span></li>
<li><span style="font-size: medium; color: #000000;">Case the segment selection as a semi-supervised regression problem</span></li>
<li><span style="font-size: medium; color: #000000;">Demonstrate that our approach provides an effective alternative for manually labeled contours</span></li>
</ul>
</li>
</ul>
<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Core Techniques</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>Multiple Tight Segment Generation: </strong>Present an algorithm that automatically generates a set of tight segments for the bounding box of an object, and at least one of these tight segments would approach the object segment</span></li>
<li><span style="font-size: medium; color: #000000;"><strong>Segment Selection: </strong>Given a few contours as well as a set of bounding boxes of an object class, we illustrate how to infer the object segments of these bounding boxes by solving a semi-supervised regression problem</span></li>
</ul>
<p><span style="font-size: medium;"><img src="images/research/ss/ss_2.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>
<table border="0" cellspacing="0" cellpadding="10" style="text-align: justify;">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/accv-cheng12.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6>Knowledge Leverage from Contours to Bounding Boxes: A Concise Approach to Annotation</h6>
Jie-Zhi Cheng, Feng-Ju Chang, Kuang-Jui Hsu, and Yen-Yu Lin
<br />
<em>Asian Conference on Computer Vision (ACCV), Lecture Notes in Computer Science, November 2012, Poster presentation (Acceptance rate: 23.2%)</em>
<br />
<a class="readon" href="images/paper/accv-cheng12.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 			
				  
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>       
    <!-- Modal 4 -->
    <div class="portfolio-modal modal fade" id="TransferLearning" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>Transfer Learning</h3>
				  
<table style="border-collapse: collapse; width: 100%; text-align: justify;" border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td><img src="images/research/tl/tl_1.png" border="0" alt="" width="800" style="display: block; margin-left: auto; margin-right: auto;" />
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><span style="font-size: medium; font-weight: bold; color: #000000;"> The cost of data labeling for image recognition or classification is often expensive. To reduce the labeling effort, transfer learning has been demonstrated to be a promising technique for object recognition with few training samples. It delivers useful knowledge in the source to improve the target model learning. We particularly focus on transferring knowledge from </span> <span style="color: #ff9900;"><span style="font-size: medium; font-weight: bold;"> <em>multiple classes to multiple classes</em></span></span><span style="font-size: medium; font-weight: bold; color: #000000;">, given two multi-class recognition tasks (one in the </span> <span style="color: #ff9900;"><span style="font-size: medium; font-weight: bold;"> <em>source domain</em></span></span><span style="font-size: medium; font-weight: bold; color: #000000;"> and the other in the </span><span style="color: #ff9900;"> <span style="font-size: medium; font-weight: bold; font-style: italic;"> target domain</span></span><span style="font-size: medium; font-weight: bold; color: #000000;">), leveraging the extra source knowledge to learn a more robust multi-class classifier rather than a set of binary classifiers in the target domain.</span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">　</p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Introduction</span></strong></p>
<ul>
<li><strong><span style="font-size: medium;"><span style="color: #000000;">Multi-class object recognition with</span> <span style="color: #ff9900;">few labeled data</span></span></strong>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>Goal</strong>: Learn a target classifier with low generalization errors</span></li>
<li><span style="font-size: medium; color: #000000;"><strong>Difficulty: </strong>When only few labeled data are available, over-fitting occurs. That is, the yielded classifier has poor generalization.</span></li>
</ul>
</li>
<li><strong><span style="font-size: medium; color: #000000;">What and how prior knowledge help to learn a robust classifier without labeling new data?</span></strong>
<ul>
<li><span style="font-size: medium; color: #000000;">Source task: many existed labeled data</span></li>
<li><span style="font-size: medium; color: #000000;">Target task: few labeled data</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Motivations: </strong>to leverage the extra source knowledge, together with the target knowledge in a common domain, and consequently learn a more robust multi-class classifier</span></li>
<li><span style="font-size: medium; color: #000000;"><strong>Conventional transfer learning algorithms:</strong> lack multi-class formulation</span></li>
</ul>
<p><span style="font-size: medium;"> <img src="images/research/tl/tl_2.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Core </span> <span style="font-size: x-large; color: #ffcc00;">Ideas</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;">Attribute transfer</span></li>
<li><span style="font-size: medium; color: #000000;">Multi-classes (source) to multi-classes (target) knowledge transfer</span></li>
<li><span style="font-size: medium; color: #000000;"><strong>What to transfer: </strong>a sequence of learnable, discriminant attributes</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Commonly shared by the source and target domains</span></li>
<li><span style="font-size: medium; color: #000000;">Converted two multi-class classification tasks to related, binary ones</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>How to transfer: </strong>Two-layer multi-task variant of AdaBoost.OC</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Boosting algorithm with error-correcting output codes (ECOC)</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Better generalization</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Outer layer: </strong>Attribute partition discovery</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Discriminant:Multi-class formulation</span></li>
<li><span style="font-size: medium; color: #000000;">Learnable:Without human effort</span></li>
<li><span style="font-size: medium; color: #000000;">Complementary:Iterative error minimization</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Inner layer: </strong>Attribute classifier learning</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Employ classifier sharing principle</span></li>
<li><span style="font-size: medium; color: #000000;">Support multiple kernel learning: Combining various low-level features</span></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><span style="font-size: medium;"> <img src="images/research/tl/tl_3.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>Our goal: </strong>Attributes should be learnable</span></li>
</ul>
<p><span style="font-size: medium;"> <img src="images/research/tl/tl_4.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>
<table border="0" cellspacing="0" cellpadding="10" style="text-align: justify;">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/accv-chang12.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6>Cross-Database Transfer Learning via Learnable and Discriminant Error-correcting Output Codes</h6>
Feng-Ju Chang, Yen-Yu Lin, and Ming-Fang Weng
<br />
<em>Asian Conference on Computer Vision (ACCV), Lecture Notes in Computer Science, November 2012,<span style="color: #00ccff;"> (Oral presentation; Acceptance rate 3.6% (31/869))</span></em>
<br />
<a class="readon" href="images/paper/accv-chang12.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 				
				  
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>


	<!-- Projocrs Part2 Modals  Projocrs Part2 Modals  Projocrs Part2 Modals  Projocrs Part2 Modals -->
    <!-- Bootstrap core JavaScript -->
    <script src="./vendor/jquery/jquery.min.js?v20191121"></script>
    <script src="./vendor/bootstrap/js/bootstrap.bundle.min.js?v20191121"></script>

    <!-- Plugin JavaScript -->
    <script src="./vendor/jquery-easing/jquery.easing.min.js?v20191121"></script>

    <!-- Contact form JavaScript -->
    <script src="./js/jqBootstrapValidation.js?v20191121"></script>
    <script src="./js/contact_me.js?v20191121"></script>

    <!-- Custom scripts for this template -->
    <script src="./js/agency.min.js?v20191121"></script>
    
	<script src="./js/jquery.magnific-popup.js?v20191121" type="text/javascript"></script>
	<script type="text/javascript" src="./js/modernizr.custom.53451.js?v20191121"></script> 
	<script src="https://momentjs.com/downloads/moment.min.js?v20191121"></script>
	<script src="https://momentjs.com/downloads/moment-timezone-with-data.min.js?v20191121"></script>
	
	<script type="text/javascript">
    	$(document).ready(function() {
			var bgImgUrl = 'img/bg{num}.jpg', bgNum,bgImgArr = [];
			for (var i=1; i <= 1; i++){
				bgImgArr.push(bgImgUrl.replace('{num}', i));
			}
//			$('header.masthead').css('background-image', 'url(img/bg3.jpg)');
			function displayTime() {
				if(!bgNum || bgNum >= bgImgArr.length) bgNum = 0;
				$('header.masthead').css('background-image', 'url('+ bgImgArr[bgNum] +')');
				bgNum++;
			}
			// Runs the displayTime function the first time
			displayTime();
			// Runs the displayTime function every second.
			setInterval(displayTime, 10000);
    	});
	</script>
 	<script>
		$(document).ready(function() {
		$('.popup-with-zoom-anim').magnificPopup({
			type: 'inline',
			fixedContentPos: false,
			fixedBgPos: true,
			overflowY: 'auto',
			closeBtnInside: true,
			preloader: false,
			midClick: true,
			removalDelay: 300,
			mainClass: 'my-mfp-zoom-in'
		});

		});
	</script>
	<script src="./js/bibtex_js.js?v20191121"></script>
	<bibtex src="publications_list.bib?v=20180628"></bibtex>
	<!--  Table -->
	<script type="./application/x-javascript"> addEventListener("load", function() { setTimeout(hideURLbar, 0); }, false); function hideURLbar(){ window.scrollTo(0,1); } </script>
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js?v20191121"></script>
	<!--webfonts-->
	<link href="https://fonts.googleapis.com/css?family=Lato:100,300,400,700,900,100italic,300italic,400italic,700italic" rel="stylesheet" type="text/css">
	<script src="./js/jquery.magnific-popup.js?v20191121"></script>
	<script src="./js/modernizr.custom.53451.js?v20191121"></script>
 	
<!--  	<bibtex src="publications.bib"></bibtex>-->
 
  </body>

</html>
