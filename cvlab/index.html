<!DOCTYPE html>
<html lang="en"><head>
	<link rel="icon" href="favicon.ico" type="image/x-icon" />
	<meta HTTP-EQUIV="EXPIRES" CONTENT="0">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="Akai and Hank">
    <title>Vision and Learning Lab</title>

    <!-- Bootstrap core CSS -->
    <link href="./vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="./vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Kaushan+Script" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css">

    <!-- Custom styles for this template -->
    <link  href="css/agency.css" rel="stylesheet">
    <link  href="./css/style.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">Vision and Learning Lab</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fa fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav text-uppercase ml-auto">
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#about">About</a>
			</li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#news">News</a>
			</li>      
          
			<li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#portfolio">Research</a>
            </li>

            <li class="nav-item"> 
              <a class="nav-link js-scroll-trigger" href="#publications">Publications</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#team">Team</a>
            </li>
       
            <li class="nav-item"> 
              <a class="nav-link js-scroll-trigger" href="#theses">Theses</a>
            </li>

            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#contact">Contact</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Header -->
    <header class="masthead" id="mastheadbg">
      <div class="container">
       
        <div class="intro-text">
		<br><br><br>
          <div class="intro-lead-in">Welcome To Vision and Learning Lab!</div>
<!--          <div class="intro-heading text-uppercase">It's Nice To Meet You</div>-->
<!--          <div id="clock"></div>-->
          <a class="btn btn-primary btn-xl text-uppercase js-scroll-trigger" href="#about">About us</a>
        </div>
      </div>
    </header>

    <!-- About -->
    <section id="about">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Vision and Learning Lab</h2>
            <p class="text-muted" style="text-align: justify;font-size: large">The Vision and Learning Lab (VLLab) at Department of Computer Science, National Chiao Tung University was founded in 2019, under the supervision of Dr. Yen-Yu Lin. We are dedicated to the development of technologies for computer vision, pattern recognition, machine learning, deep learning, artificial intelligence and multimedia systems.</p>
         <iframe src="photoslide.html" width="70%" height="500px" frameborder="0" scrolling="no"></iframe>
          </div>
        </div>
        <br>
      </div>
    </section>
    
    <!-- News -->
    <section class="bg-light" id="news">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">RECENT NEWS</h2><br>
          </div>
        </div>
 		<div class="row">
			<div class="[ col-xs-12 col-sm-offset-2 col-sm-11 ]" style="margin: 0 auto">
				<ul class="event-list" style="width: 100%">
					<li><time datetime="2019-9-4">
							<span class="day">4</span>
							<span class="month">Sep</span>
							<span class="year">2019</span>
							<span class="time">Evening</span></time>
						<div class="info"><div class="row"><div class="col-sm-12">
							<p style="font-size: 16px"><strong>Congratulations! Our paper was accepted by NeurIPS 2019</strong><br>
							Cheng-Chun Hsu*, Kuang-Jui Hsu*, Chung-Chi Tsai, Yen-Yu Lin, and Yung-Yu Chuang, "Weakly Supervised Instance Segmentation using the Bounding Box Tightness Prior," Thirty-third Conference on Neural Information Processing Systems (NeurIPS), December 2019.
							</a></p>							
							</div></div></div></li>

					<li><time datetime="2019-8-23">
							<span class="day">23</span>
							<span class="month">Aug</span>
							<span class="year">2019</span>
							<span class="time">Evening</span></time>
						<div class="info"><div class="row"><div class="col-sm-12">
							<p style="font-size: 16px"><strong>Congratulations! Our paper was accepted by IJCV 2019</strong><br>
							Yi-Wen Chen, Yi-Hsuan Tsai, Yen-Yu Lin, and Ming-Hsuan Yang, "VOSTR: Video Object Segmentation via Transferable Representations," to appear in International Journal of Computer Vision (IJCV).
							</a></p>							
							</div></div></div></li>

					<li><time datetime="2019-8-20">
							<span class="day">20</span>
							<span class="month">Aug</span>
							<span class="year">2019</span>
							<span class="time">Evening</span></time>
						<div class="info"><div class="row"><div class="col-sm-12">
							<p style="font-size: 16px"><strong>Congratulations! Our paper was accepted by ICCV 2019</strong><br>
							Yu-Jhe Li, Yun-Chun Chen, Yen-Yu Lin, Xiaofei Du, and Yu-Chiang Frank Wang, "Recover and Identify: A Generative Dual Model for Cross-Resolution Person Re-Identification," IEEE International Conference on Computer Vision (ICCV), October 2019.  
							</a></p>							
							</div></div></div></li>
							
					<li><time datetime="2019-8-20">
							<span class="day">20</span>
							<span class="month">Aug</span>
							<span class="year">2019</span>
							<span class="time">Evening</span></time>
						<div class="info"><div class="row"><div class="col-sm-12">
							<p style="font-size: 16px"><strong>Congratulations! Our paper was accepted by TMM 2019</strong><br>
							Chung-Chi Tsai, Kuang-Jui Hsu, Yen-Yu Lin, Xiaoning Qian, and Yung-Yu Chuang, "Deep Co-saliency Detection via Stacked Autoencoder-enabled Fusion and Self-trained CNNs," to appear in IEEE Transactions on Multimedia (TMM). 
							</a></p>							
							</div></div></div></li>
							
					<li><time datetime="2019-8-1">
							<span class="day">1</span>
							<span class="month">Aug</span>
							<span class="year">2019</span>
							<span class="time">Evening</span></time>
						<div class="info"><div class="row"><div class="col-sm-12">
							<p style="font-size: 18px">	<br><strong>Our website is online!</a></strong></p>
							</div></div></div></li>
				</ul>
			</div>
     	</div>
     	
	
      </div>
    </section>
    

<!-- Projects Projects Projects Projects Projects Projects -->

    <section id="portfolio">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Research</h2>
            <h3 class="section-subheading text-muted"></h3>
          </div>
        </div>
        <div class="row">
           <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#ActionRecognition">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fa fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/ar.jpg" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>Action Recognition</h4>
            </div>
          </div>
         <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#MultimediaRetrieval">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fa fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/ir.jpg" width="98%" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>Content-based Multimedia Retrieval</h4>
            </div>
          </div>
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#Classification">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fa fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/cf.jpg" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>Data Classification and Clustering</h4>

            </div>
          </div>
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#FaceDetection">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fa fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/fd.jpg" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>Face Detection</h4>
            </div>
          </div>
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#FeatureMatching">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fa fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="http://cvlab.citi.sinica.edu.tw/images/thumbs/cvpr-chen13.png" width="98%" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>Feature Matching</h4>
            </div>
          </div>
 
           <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#ObjectRecognition">
              <div class="portfolio-hover"><div class="portfolio-hover-content"><i class="fa fa-plus fa-3x"></i></div></div>
              <img class="img-fluid" src="img/portfolio/or.jpg" alt="">
            </a>
            <div class="portfolio-caption"><h4>Object Recognition</h4></div></div>
                          
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#PeopleCounting">
              <div class="portfolio-hover"><div class="portfolio-hover-content"><i class="fa fa-plus fa-3x"></i></div></div>
              <img class="img-fluid" src="img/portfolio/pc.jpg" alt="">
            </a>
            <div class="portfolio-caption"><h4>People Counting</h4></div></div>        
          
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#SemanticSegmentation">
              <div class="portfolio-hover"><div class="portfolio-hover-content"><i class="fa fa-plus fa-3x"></i></div></div>
              <img class="img-fluid" src="img/portfolio/ss.jpg" alt="">
            </a>
            <div class="portfolio-caption"><h4>Semantic Segmentation</h4></div></div>         
          
          
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#TransferLearning">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fa fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/tl.jpg" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>Transfer Learning</h4>
            </div>
          </div>

        </div>
      </div>
    </section>

	<!-- Projects Projects Projects Projects Projects Projects -->
    
	<!-- Publications Publications Publications Publications-->

    <section class="bg-light" id="publications">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading" >PUBLICATIONS</h2>
                    <!--<h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3>-->
                </div>
            </div>
            
            <div class="bibtex_structure">
  				<div class="group year" extra="ASC number">
  	 			<div style="padding-bottom:10px;"></div>
  	  			<div class="sort journal" extra="DESC string">
      			<div class="templates"></div></div></div></div>

			<div id="bibtex_display">
  				<div class="if bibtex_template" style="display: none;">
    				<ul> <li>
     				
      				<span class="if journal !nolink">
            			<div class="if journal" style="background:#EFB326;padding:0.1em 0.3em;border-radius: 0.4em; display:inherit">
            				<span class="bibtextype" style=";color: white"></span></div>
            			<span class="if !journal" style="background:#0769CE;;padding:0.1em 0.3em;border-radius: 0.4em; display:inherit">
            				<span class="bibtextype" style=";color: white"></span></span>
     						<a class="bibtexVar" href="+URL+" extra="URL" target="_blank" style="color:black;"><span style="font-weight: bold;" class="title"></span></a>
							<span class="if address">[<a class="bibtexVar" href="+ADDRESS+" extra="ADDRESS" target="_blank" style="color:orange;"><storng>Web|Code</storng></a>]</span>
<!--						            			<span style="font-weight: bold;" class="title"></span>-->
        			</span>
      				<span class="if title nolink">
            			<span style="font-weight: bold;" class="title"></span>
      				</span>
      				<div class="if author">
        			<span class="author"></span></div>
					<div>
						<span class="if journal"><em><span class="journal"></span></em>,</span>
						<span class="if booktitle"><em><span class="booktitle"></span></em>,</span>
						<span class="if editor"><span class="editor"></span> (editors),</span>
<!--						<span class="if publisher"><em><span class="publisher"></span></em>,</span>-->
<!--						<span class="if !journal number">Technical report <span class="number"></span>,</span>-->
<!--						<span class="if institution"><span class="institution"></span>,</span>-->
<!--						<span class="if address"><span class="address"></span>,</span>-->
						<span class="if volume"> volume <span class="volume"></span>,</span>
						<span class="if journal number"> number <span class="number"></span>,</span>
						<span class="if pages"> pages <span class="pages"></span>,</span>
						<span class="if month"><span class="month"></span>,</span>
						<span class="if year"><span class="year"></span>.</span>
						<span class="if note"><span class="note"></span>.</span>

<!--						<span class="if url" style="margin-left: 10px">
     						[<a class="bibtexVar" href="+URL+" extra="URL" target="_blank" style="color:orange;"><storng>PDF</storng></a>]
    					</span>-->
					</div>
	 				
    				</li></ul>
				</div>
			</div>
  		
   		</div>
    </section>
	<!-- Publications Publications Publications Publications-->
    <!-- Team -->
    <!-- ￣￣￣￣￣￣￣￣￣￣￣￣￣￣Member ￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣-->    
    <section id="team">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Our Team</h2>
            <!--            <h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3>-->
          </div>
        </div>
     <!-- Pi -->   

<!-- One Person -->
<h3>Laboratory Director</h3>       
<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><a target="_blank" href="https://sites.google.com/site/yylinweb/"><img class="mx-auto rounded-circle" src="http://cvlab.citi.sinica.edu.tw/images/member/yylin.jpg"  border="1" alt="" width="200" /></a></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Dr. Yen-Yu Lin </h4> 
  [<a target="_blank" href="https://sites.google.com/site/yylinweb/" style="color: #00ccff;">Website</a>]  <p style="margin-top: 5px; margin-bottom: 5px;"><em><span>Ph.D., Department of Computer Science and Information Engineering, National Taiwan University.</span></em></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
<li>Machine Learning</li>
<li>Deep Learning</li>
<li>Artificial Intelligence</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: lin[at]cs.nctu.edu.tw </li>
  <li>Tel: +886-3-5712121 ext.54781</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>　</p>

<h3>Research Assistants </h3>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="http://cvlab.citi.sinica.edu.tw/images/member/cchsu.bmp"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Cheng-Chun Hsu</h4> 
  <p style="margin-top: 5px; margin-bottom: 5px;"><em><span>Bachelor, Department of Computer Science and Information Engineering, National Taiwan University of Science and Technology.</span></em></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>

</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: hsu06118[at]iis.sinica.edu.tw </li>
  <li>Tel: +886-2-2787-2300 ext.2330</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/hywang.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Han-Yi Wang </h4> 
  <p style="margin-top: 5px; margin-bottom: 5px;"><em><span>M.S., Department of Computer Science and Information Engineering, National Cheng Kung University.</span></em></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
<li>Machine Learning</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: wanghy917[at]gmail.com </li>
  <li>Tel: +886-2-2787-2300 ext.2329</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>　</p>


<h3>Ph.D. Students</h3>
<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/hyl.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Han-Yi Lin</h4> 
  <p style="margin-top: 5px; margin-bottom: 5px;"><em><span>Ph.D. student, Department of Computer Science and Information Engineering, National Taiwan University.</span></em></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
<li>Deep Learning</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: d03922006[at]csie.ntu.edu.tw </li>
  <li>Tel: +886-2-2787-2300 ext.2359</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/ylliu.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Yu-Lun Liu</h4> 
  <p style="margin-top: 5px; margin-bottom: 5px;"><em><span>Ph.D. student, Department of Computer Science and Information Engineering, National Taiwan University.</span></em></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
<li>Deep Learning</li>
<li>Video Frame Interpolation</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>Tel: +886-2-2787-2300 ext.2329</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/cky.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Cheng-Kun Yang</h4> 
  <p style="margin-top: 5px; margin-bottom: 5px;"><em><span>Ph.D. student, Department of Computer Science and Information Engineering, National Taiwan University.</span></em></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
<li>Deep Learning</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>E-mail: d08922002[at]ntu.edu.tw </li>
<!--  <li>Tel: +886-2-2787-2300 ext.2330</li>-->
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>　</p>


<h3>M.S. Students</h3>
<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="img/team/yhhuang.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Yi-Hsuan Huang</h4> 
  <p style="margin-top: 5px; margin-bottom: 5px;"><em><span>M.S. Student, Graduate Institute of Networking and Multimedia, National Taiwan University.</span></em></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>Tel: +886-2-2787-2300 ext.2329</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="5" bgcolor="#ffffff">
<tbody>
<tr>
<td>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="8">
<tbody>
<tr>
<td rowspan="2" valign="middle" width="200"><img class="mx-auto rounded-circle" src="http://cvlab.citi.sinica.edu.tw/images/member/ytliou.jpg"  border="1" alt="" width="200" /></td>
<td colspan="2" valign="middle" width="1170"><h4 style="display:inline">Yan-Ting Liou</h4> 
  <p style="margin-top: 5px; margin-bottom: 5px;"><em><span>M.S. Student, Department of Computer Science and Information Engineering, National Taiwan University.</span></em></p>
</td>
</tr>
<tr>
<td valign="top" width="30%"><span style="color: #000000;"><strong>Research Interests:</strong></span>
  <ul>
<li>Computer Vision</li>
<li>Rendering</li>
</ul>
</td>
<td valign="top" width="70%"><strong>Contact Information:</strong>
  <ul>
  <li>Tel: +886-2-2787-2300 ext.2329</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>　</p>


       <div class="row">
	   

          <!--next-->
        </div> 
      <!-- _______________ _______Member______________________-->                                                                 
     <!-- ￣￣￣￣￣￣￣￣￣￣￣￣￣￣Alumni ￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣-->     
       
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Alumni</h2>
          </div>
        </div>

		<div class="row">
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/kjhsu.jpg" alt="">
              <h5>Kuang-Jui Hsu </h5>Duration: Mar. 2011 ~ Jul. 2019.
			  <br>Current status: Qualcomm.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/wcwang.jpg" alt="">
              <h5>Wei-Cheng Wang  </h5>Duration: Mar. 2018 ~ Jul. 2019.
			  <br>Current status: Ph.D. Student, Computer Science Engineering, Ghent University.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ytchen.jpg" alt="">
              <h5>Yi-Ting Chen </h5>Duration: Jul. 2018 ~ Jul. 2019.
			  <br>Current status: M.S. student, Master of Science in Computer Vision, Robotics Institute, Carnegie Mellon University.
            </div></div>
			
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/yhhuang_2.jpg" alt="">
              <h5>Yung-Han Huang  </h5>Duration: Jul. 2017 ~ Jun. 2019.
			  <br>Current status: M.S. student, Data Science Degree Program, National Taiwan University.
            </div></div>
			
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/cayang.jpg" alt="">
              <h5>Chiao-An Yang  </h5>Duration: Jul. 2017 ~ Jun. 2019.
			  <br>Current status: M.S. student, Department of Computer Science and Information Engineering, National Taiwan University.
            </div></div>
			
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ywchen.jpg" alt="">
              <h5>Yi-Wen Chen  </h5>Duration: Jul. 2017 ~ Jun. 2019.
			  <br>Current status: Ph.D. Student, Electrical Engineering and Computer Science, University of California, Merced.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/hhhung.jpg" alt="">
              <h5>Hao-Hsiang Hung  </h5>Duration: Jul. 2017 ~ Mar. 2019.
			  <br>Current status: Undergraduate student, Department of Computer Science and Information Engineering, National Taiwan University.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ccho.jpg" alt="">
              <h5>Chia-Chen Ho   </h5>Duration: Jul. 2018 ~ Mar. 2019.
			  <br>Current status: Research Assistant, Department of Computer Science, National Tsing Hua University.
            </div></div>
        
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ycchen.png" alt="">
              <h5>Yun-Chun (Johnny) Chen   </h5>Duration: Jul. 2018 ~ Mar. 2019.
			  <br>Current status: Short-term Visiting Scholar, Bradley Department of Electrical and Computer Engineering, Virginia Tech.
            </div></div>
			
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/yccheng.jpg" alt="">
              <h5>Yu-Chiang Cheng   </h5>Duration: Sep 2017 ~ Feb. 2019.
			  <br>Current status: Bachelor, Department of Electrical Engineering, National Taiwan University.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ctchou.png" alt="">
              <h5>Chao-Te Chou   </h5>Duration: Jul. 2017 ~ Feb. 2019.
			  <br>Current status: Military Service.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/tyyang.jpg" alt="">
              <h5>Tsun-Yi Yang   </h5>Duration: Jul. 2014 ~ Jan. 2019.
			  <br>Current status: Research Intern, Scape Technologies.
            </div></div>

		<div class="row">
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ymyeh.png" alt="">
              <h5>Yang-Ming Yeh   </h5>Duration: Sep. 2015 ~ Jul. 2018.
			  <br>Current status: Ph.D. student, Graduate Institute of Electronics Engineering, National Taiwan University.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/tkhu.jpg" alt="">
              <h5>Ting-Kuei Hu   </h5>Duration: Sep. 2015 ~ Jul. 2018.
			  <br>Current status: M.S. Student, Computer Science & Engineering, Texas A&M University.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ytliao.jpg" alt="">
              <h5>Yi-Tung Liao   </h5>Duration: Sep. 2016 ~ Jul. 2018.
			  <br>Current status: World Quant.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ykchiu.jpg" alt="">
              <h5>Yu-Kai Chiu   </h5>Duration: May. 2018 ~ Jul. 2018.
			  <br>Current status: M.S. student, Master of Entertainment Technology, School of Computer Science, Carnegie Mellon University.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/cctsai.jpg" alt="">
              <h5>Chung-Chi Tsai   </h5>Duration: May. 2016 ~ Jun. 2018.
			  <br>Current status: Qualcomm
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/lyyu.jpg" alt="">
              <h5>Li-Yu Yu   </h5>Duration: Jul. 2016 ~ Nov. 2017.
			  <br>Current status: M.S. student, EE, NTU.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/pxhuang.jpg" alt="">
              <h5>Po-Hsiang Huang   </h5>Duration: Jul. 2016 ~ Nov. 2017.
			  <br>Current status: Undergraduate, EE, NTU.
            </div></div>
			
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/fychuang.jpg" alt="">
              <h5>Fu-Yu Chuang   </h5>Duration: Jul. 2016 ~ Sep. 2017.
			  <br>Current status: M.S. student, EE, NTU.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/cyyang.jpg" alt="">
              <h5>Chu-Ya Yang   </h5>Duration: Jul. 2016 ~ Aug. 2017.
			  <br>Current status: Research Assistant, IIS, Academia Sinica -> M.S. Student, Computer Science, Texas A&M University.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/yfshih.jpg" alt="">
              <h5>Ya-Fang Shih   </h5>Duration: Mar. 2015 ~ Aug. 2017.
			  <br>Current status: HTC -> Verisk Analytics.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/jhhsu.png" alt="">
              <h5>Jo-Han Hsu   </h5>Duration: Sep. 2015 ~ Jun. 2017.
			  <br>Current status: HTC.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/hhchang.jpg" alt="">
              <h5>Hao-Hsuan Chang   </h5>Duration: Sep. 2016 ~ Jun. 2017.
			  <br>Current status: Ph.D. Student, Electrical and Computer Engineering, Virginia Tech.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ananyu.jpg" alt="">
              <h5>An An Yu   </h5>Duration: Jan. 2016 ~ Jun. 2016.
			  <br>Current status: Google.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/wrliu.jpg" alt="">
              <h5>Wan-Rou Liu   </h5>Duration: Jan. 2016 ~ May 2016.
			  <br>
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/phhsiao.jpg" alt="">
              <h5>Pai-Heng Hsiao   </h5>Duration: Nov. 2013 ~ Dec. 2015.
			  <br>Current status: Umbo cv -> Memorence AI
            </div></div>
			
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/wslai.jpg" alt="">
              <h5>Wei-Sheng Lai   </h5>Duration: Sep. 2014 ~ Jul. 2015.
			  <br>Current status: Ph.D. Student, Electrical Engineering and Computer Science, University of California, Merced.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/yuhu.jpg" alt="">
              <h5>Yuan-Ting Hu   </h5>Duration: Jun. 2013 ~ Jun. 2015.
			  <br>Current status: ULSee -> Ph.D. Student, Electrical and Computer Engineering, University of Illinois Urbana-Champaign.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/hhsu.jpg" alt="">
              <h5>Hsiao-Hang Su   </h5>Duration: Jul. 2014 ~ Apr. 2015.
			  <br>Current status: Undergraduate Student, School of Dentistry, Kaohsiung Medical University.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/hytsai.jpg" alt="">
              <h5>Han-Yi Tsai   </h5>Duration: Jun. 2013 ~ Dec. 2014.
			  <br>Current status: Dynacolor.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/mhchen.jpg" alt="">
              <h5>	Min-Hung Chen  </h5>Duration: Jul. 2013 ~ Jul. 2014.
			  <br>Current status: Ph.D. Student, Electrical and Computer Engineering, Georgia Institute of Technology.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/htchen.jpg" alt="">
              <h5>Hsiao-Tung Chen   </h5>Duration: Jul. 2013 ~ Jul. 2014.
			  <br>Current status:  M.S. Student, Electrical and Computer Engineering, Cornell University -> Amazon.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/hychen.jpg" alt="">
              <h5>	Hsin-Yi Chen  </h5>Duration: Jul. 2011 ~ Jun. 2014.
			  <br>Current status: Ph.D. Student, Computer Science and Information Engineering, National Taiwan University -> Tesla.
            </div></div>


          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/fjchang.jpg" alt="">
              <h5>Feng-Ju Chang  </h5>Duration: Aug. 2011 ~ Jul. 2013.
			  <br>Current status: Ph.D. Student, Electrical Engineering, University of Southern California.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ycchang.jpg" alt="">
              <h5>Yao-Chuan Chang   </h5>Duration: Jun. 2012 ~ Jun. 2013.
			  <br>Current status: Ph.D. Student, Biomedical Engineering, University of Southern California.
            </div></div>
			
          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/jttsai.jpg" alt="">
              <h5>Jeng-Tsung Tsai   </h5>Duration: Jul. 2012 ~ Jun. 2013.
			  <br>Current status: M.S. Student, Computer Science, University of Southern California -> MathWorks.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/jhhua.jpg" alt="">
              <h5>Ju-Hsuan Hua   </h5>Duration: Aug. 2012 ~ Jun. 2013.
			  <br>Current status: M.S. Student, Robotics, Carnegie Mellon University.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/ycwang.jpg" alt="">
              <h5>Yi-Chen Wang   </h5>Duration: Jul. 2011 ~ Jun. 2012.
			  <br>Current status: TSMC.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/calin.jpg" alt="">
              <h5>Chin-An Lin   </h5>Duration: Jul. 2011 ~ Jun. 2012.
			  <br>Current status: Quanta Research Institute.
            </div></div>

          <div class="col-sm-3">
            <div class="alumni">
              <img class="mx-auto rounded-circle" src="images/member/tylin.png" alt="">
              <h5>Tsung-Yi Lin   </h5>Duration: Feb. 2011 ~ Jul. 2011.
			  <br>Current status: Ph.D. Student, Computer Science, University of California, San Diego.
            </div></div>

 
        </div>         
       
        
    
     <!-- _______________ _______Alumni______________________-->                                                                 
      </div>
    </section>

    <!-- About -->
    <section id="theses">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
															 
<h2>Ph.D. Thesis</h2>
<table border="0" cellspacing="0" cellpadding="10"  style="text-align: justify;">
<tbody>
<tr>
<td valign="top">
<h5><a class="readon" href="images/thesis/phdthesis-hsu19.pdf" target="_blank" style="color: #000000;">Visual Attention-getting Object Discovery via Learning Weakly Supervised CNNs</a></h5>
Author: Kuang-Jui Hsu<br>
Institute: Computer Science and Information Engineering, National Taiwan University<br>
Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin<br>
Date: July, 2019<br>
</td>
</tr>
<tr>
<td valign="top">
<h5><a class="readon" href="images/thesis/phdthesis-tsai19.pdf" target="_blank"  style="color: #000000;">Image Co-saliency Detection: Novel Approaches with Convex Optimization and Deep Neural Networks</a></h5>
Author: Chung-Chi Tsai<br>
Institute: Electrical Engineering, Texas A&amp;M University<br>
Advisors: Dr. Xiaoning Qian and Dr. Yen-Yu Lin<br>
Date: August, 2018<br>
</td>
</tr>
<tr>
<td valign="top">
<h5>Human Action Recognition based on Probabilistic Graphical Models</h5>
Author: Shih-Yao Lin<br>
Institute: Graduate Institute of Networking and Multimedia, National Taiwan University<br>
Advisors: Dr. Yi-Ping Hung, Dr. Chu-Song Chen, and Dr. Yen-Yu Lin<br>
Date: January, 2016<br>
Award: <em><span style="font-size: 13px; line-height: 1.3em; color: #000000;">Ph.D. Thesis Award, Institute of Information &amp; Computing Machinery (IICM)</span></em><br>
</td>
</tr>
<tr>
<td valign="top">
<h5><a class="readon" href="images/thesis/phdthesis-lin10.pdf" target="_blank"  style="color: #000000;">Multiple Kernel Learning for Computer Vision Applications </a></h5>
Author: Yen-Yu Lin<br>
Institute: Computer Science and Information Engineering, National Taiwan University<br>
Advisors: Dr. Tyng-Luh Liu and Dr. Chiou-Shann Fuh<br>
Date: October, 2010<br>
</td>
</tr>
</tbody>
</table>
<h2>　</h2>
<h2>Master Thesis</h2>
<table border="0" cellspacing="0" cellpadding="10"  style="text-align: justify;">
<tbody>
<tr>
<td valign="top">
<h5><a class="readon" href="images/thesis/masterthesis-liao18.pdf" target="_blank"  style="color: #000000;">Deep Video Frame Interpolation using Cyclic Frame Generation</a></h5>
Author: Yi-Tung Liao<br>
Institute: Computer Science and Information Engineering, National Taiwan University<br>
Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin<br>
Date: July, 2018<br>
Award: <em><span style="font-size: 13px; line-height: 1.3em; color: #000000;">Master Thesis Award, Institute of Information Computing Machinery (IICM)</span></em><br>
</td>
</tr>
<tr>
<td valign="top">
<h5><a class="readon" href="images/thesis/masterthesis-shih17.pdf" target="_blank"  style="color: #000000;">Deep Co-occurrence Feature Learning for Visual Object Recognition</a></h5>
Author: Ya-Fang Shih<br>
Institute: Computer Science and Information Engineering, National Taiwan University<br>
Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin<br>
Date: July, 2017<br>
Award: <em><span style="font-size: 13px; line-height: 1.3em; color: #000000;">Master Thesis Award, The Chinese Image Processing and Pattern Recognition Society; Master Thesis Award, Institute of Information Computing Machinery (IICM)</span></em><br>
</td>
</tr>
<tr>
<td valign="top">
<h5>Distilling from the Past: Self-Distillation by Using Epoch Regularization</h5>
Author: Jo-Han Hsu<br>
Institute: Computer Science and Information Engineering, National Taiwan University<br>
Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin<br>
Date: July, 2017<br>
</td>
</tr>
<tr>
<td valign="top">
<h5>Patch Match with Multiple Descriptors for Scene Alignment</h5>
Author: Han-Yi Tsai<br>
Institute: Computer Science and Information Engineering, National Taiwan University<br>
Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin<br>
Date: June, 2014<br>
</td>
</tr>
<tr>
<td valign="top">
<h5>DescriptorBoost: An Unsupervised Approach to Fusing Multiple Descriptors in the Homography Space</h5>
Author: Yuan-Ting Hu<br>
Institute: Computer Science and Information Engineering, National Taiwan University<br>
Advisors: Dr. Bing-Yu Chen and Dr. Yen-Yu Lin<br>
Date: June, 2014<br>
</td>
</tr>
<tr>
<td valign="top">
<h5><a class="readon" href="images/thesis/masterthesis-hsu13.pdf" target="_blank"  style="color: #000000;">Augmented Multiple Instance Regression for Inferring Object Contours within Bounding Boxes</a></h5>
Author: Kuang-Jui Hsu<br>
Institute: Graduate Institute of Networking and Multimedia, National Taiwan University<br>
Advisors: Dr. Yung-Yu Chuang and Dr. Yen-Yu Lin<br>
Date: June, 2013<br>
Award: <em><span style="font-size: 13px; line-height: 1.3em; color: #000000;">Best Master Thesis Award, Institute of Information &amp; Computing Machinery (IICM)</span></em><br>
</td>
</tr>
<tr>
<td valign="top">
<h5>Anomaly Detection Localization via Foreground Motion Information</h5>
Author: Yi-Chen Wang<br>
Institute: Department of Electrical Engineering, National Taiwan University<br>
Advisors: Dr. Shyh-Kang Jeng and Dr. Yen-Yu Lin<br>
Date: June, 2012<br>
</td>
</tr>
<tr>
<td valign="top">
<h5>An Efficient Temporal Model for Action Recognition Using Multivariate Linear Prediction</h5>
Author: Chin-An Lin<br>
Institute: Graduate Institute of Communication Engineering, National Taiwan University<br>
Advisors: Dr. Shyh-Kang Jeng and Dr. Yen-Yu Lin<br>
Date: June, 2012<br>
Award: <em><span style="font-size: 13px; line-height: 1.3em; color: #000000;">Best Master Thesis Award, Institute of Information &amp; Computing Machinery (IICM)</span></em><br>
</td>
</tr>
</tbody>
</table>
<h2>　</h2>		
         
          </div>
        </div>
        <br>
      </div>
    </section>


    <!-- Contact -->
    <section id="contact">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
			  <h2 class="section-heading text-uppercase">Contact Us</h2>
          </div>
        </div>
        <div class="mainbody" style="color:whitesmoke;font-family: 'Montserrat', 'Helvetica Neue', Helvetica, Arial, sans-serif;margin-bottom: 2em">
        <h4 style="color: #fed136">Lab Location</h4>
        <ol>EC118, Engineering Building 3, 1001 University Road, Hsinchu 300, Taiwan</ol>
        <div class="col-sm-8 col-xs-12">
                                    <div class="locations_map">
                                        <iframe
                                            width="700"
                                            height="400"
                                            frameborder="0" style="border:2px-solid-blue;"
                                            src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d3622.2468276278687!2d120.99525951539745!3d24.787000254301407!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x3468360f95ae32a7%3A0xdc6638191f31d7a9!2z5Lqk6YCa5aSn5a245bel56iL5LiJ6aSo!5e0!3m2!1szh-TW!2stw!4v1568094073296!5m2!1szh-TW!2stw" allowfullscreen>
                                            </iframe>
                                    </div>
                                </div>
        </div>

      </div>
    </section>

    <!-- Footer -->
    <footer>
      <div class="container">
        <div>
       <span class="copyright">2019 © Vision and Learning Lab, <a href="https://www.cs.nctu.edu.tw/" target="_blank">National Chiao Tung University</a>. All Rights Reserved. | <a href="https://drive.google.com/file/d/1G7flFb3xFrVo1V7EZ5quLNtfRMhQ-sg3/view?usp=sharing" target="_blank">Safety Policy</a> & <a href="http://secretariat.nctu.edu.tw/tw/intellectual/" target="_blank">Privacy Policy</a></span>
        </div>
      </div>
    </footer>

  
    <!-- Oasis -->
    <div class="portfolio-modal modal fade" id="ActionRecognition" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>Action Recognition</h3>

<table style="border-collapse: collapse; width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td>

<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><span style="font-size: medium; font-weight: bold; color: #000000;"> We aim to resolve the difficulties of action recognition arising from the large intra-class variations. These unfavorable variations make it infeasible to represent one action instance by other ones of the same action. We hence propose to extract both </span> <span style="color: #ff9900;"> <span style="font-size: medium; font-weight: bold; font-style: italic;"> instance-specific</span></span><span style="font-size: medium; font-weight: bold; color: #000000;"> and </span> <span style="color: #ff9900;"> <span style="font-size: medium; font-weight: bold; font-style: italic;"> class-consistent</span></span><span style="font-size: medium; font-weight: bold; color: #000000;"> features to facilitate action recognition. Specifically, the instance-specific features explore the self-similarities among frames of each video instance, while class-consistent features summarize withinclass similarities. We introduce a generative formulation to combine the two diverse types of features. The experimental results demonstrate the effectiveness of our approach.</span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">　</p>
<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Instance-specific and Class-consistent Cues</span></strong></p>
<ul style="text-align: justify;">
<li><span style="font-size: medium; color: #000000;">We aim to resolve the difficulties of action recognition arising from the <em>large intra-class variations</em>. These unfavorable variations make it infeasible to represent one action instance by other ones of the same action. We hence propose to extract both <em>instance-specific</em> and <em>class-consistent</em> features to facilitate action recognition.</span></li>
<li><span style="font-size: medium; color: #000000;"><strong>Credits</strong></span>
<ul style="text-align: justify;">
<li><span style="font-size: medium; color: #000000;"><strong>Instance-specific features: </strong>Self-similarities among frames of an action sequence. <em>Multivariate linear prediction (MLP)</em> is adopted to aggregate all the causalities among frames. </span></li>
<li><span style="font-size: medium; color: #000000;"><strong>Class-consistent features: </strong>Characteristics shared by instances of the same action. <em>Support vector machines (SVMs)</em> are used to discover these features based on the bag-of-words model. </span></li>
<li><span style="font-size: medium; color: #000000;">We propose a <em>generative</em> formulation to integrate the two complementary types of features, and boost the performance. </span></li>
</ul>
</li>
</ul>

<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Core Techniques</span></strong></p>
<ul style="text-align: justify;">
<li><span style="font-size: medium; color: #000000;">We view actions as multivariate time signals. For signal processing in our approach, there are several essential techniques:</span>
<ul style="text-align: justify;">
<li><span style="font-size: medium; color: #000000;">Wide-Sense Stationary Process</span></li>
<li><span style="font-size: medium; color: #000000;">Linear Prediction</span></li>
<li><span style="font-size: medium; color: #000000;">Support Vector Machine</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;">We propose a generative model. The main idea is to consider the static and dynamic information of a multivariate time signal separately. This is based on the assumption that these two information are independent for action recognition.</span>
<ul style="text-align: justify;">
<li><span style="font-size: medium; color: #000000;">Instance-specific cue via MLP</span></li>
<li><span style="font-size: medium; color: #000000;">Class-consistent cue via SVM</span></li>
<li><span style="font-size: medium; color: #000000;">Linear fusion</span><br /> 　</li>
</ul>
</li>
</ul>
<p><span style="font-size: medium;"><img src="images/research/ar/ar_2.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>

<table border="0" style="text-align: left">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/icip-lin12.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6>Action Recognition using Instance-specific and Class-consistent Cues</h6>
Chin-An Lin, Yen-Yu Lin, Hong-Yuan Mark Liao, and Shyh-Kang Jeng<br>
<em>IEEE International Conference on Image Processing (ICIP), September 2012</em><br>
<a class="readon" href="images/paper/icip-lin12.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 	

                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- HR -->
    <div class="portfolio-modal modal fade" id="MultimediaRetrieval" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>Content-based Multimedia Retrieval</h3>
				  
						
<table style="border-collapse: collapse; width: 100%; text-align: justify;" border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td>
<p align="center"><img src="images/research/cbmr/cbmr_1.png" border="0" alt="" /></p>
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><span style="font-size: medium; font-weight: bold; color: #000000;">The development of image querying using content-based image retrieval (CBIR) techniques has attracted a great attention owing to its abundant applicability. We are particularly interested in how the user's semantics could be integrated into a CBIR system. The user's semantics for CBIR involves two different sources of information: the similarity relations entailed by the content-based features, and the </span><span style="color: #ff9900;"> <span style="font-size: medium; font-weight: bold; font-style: italic;">relevance relations</span></span><span style="font-size: medium; font-weight: bold; color: #000000;"> specified in the feedback. Besides, we also look into the issues of selecting a good feature set for improving the retrieval performance. </span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">　</p>
<p><span style="font-size: x-large;"><strong><span style="color: #ffcc00;"> Semantic Manifold Learning for Image Retrieval</span></strong></span></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>To address the problem of CBIR with relevance feedback by manifold learning</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Two aspects of information are being fused: <em>Intrinsic Similarity Relations</em> and <em>Query &amp; Relevance Feedback</em></span></li>
<li><span style="font-size: medium; color: #000000;">User-specific Semantic Manifold</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>A manifold-learning technique to effectively fuse the multi-modality of information </strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;"><em>Intrinsic Similarity Relations</em>: abundant, and auxiliary</span></li>
<li><span style="font-size: medium; color: #000000;"><em>User Relevance Feedback</em>: few, but crucial</span></li>
</ul>
</li>
</ul>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>User relevance feedback serves as augmented relations to intrinsic similarity relations</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Augmented Relations Embedding (ARE)</span></li>
<li><span style="font-size: medium; color: #000000;">Augmented features: global and local image features</span></li>
</ul>
</li>
</ul>
<p align="center"><img src="images/research/cbmr/cbmr_2.png" border="0" alt="" /></p>

<p><strong><span style="font-size: x-large; color: #ffcc00;">Features</span></strong></p>
<ul>
<li><span style="color: #000000; font-size: medium;"><strong>Global</strong></span>
<ul>
<li><strong><span style="color: #000000; font-size: medium;">Color</span></strong>
<ul>
<li><span style="font-size: medium; color: #000000;">Quantized HSV color histogram: 64 dimensions</span></li>
<li><span style="font-size: medium; color: #000000;">First three moments in each color channel: 9 dimensions</span></li>
<li><span style="font-size: medium; color: #000000;">Color coherence vector: 128 dimensions</span></li>
</ul>
</li>
<li><strong><span style="color: #000000; font-size: medium;">Texture</span></strong>
<ul>
<li><span style="font-size: medium; color: #000000;">Tamura coarseness: 10 dimensions</span></li>
<li><span style="font-size: medium; color: #000000;">Tamura directionality: 8 dimensions</span></li>
</ul>
</li>
<li><strong><span style="color: #000000; font-size: medium;">Wavelet</span></strong>
<ul>
<li><span style="color: #000000; font-size: medium;">3-level DWT image decomposition</span></li>
<li><span style="color: #000000; font-size: medium;">The first two moments in each high-frequency sub-bands: 18 dimensions</span></li>
</ul>
</li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Local</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Detecting salient regions (associated with interest points)</span></li>
<li><span style="font-size: medium; color: #000000;">Describing the local properties of each salient region</span></li>
<li><span style="font-size: medium; color: #000000;">Difference-of-Gaussian (DoG) for salient-region detection [Lowe 2004]</span></li>
</ul>
</li>
</ul>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>Augmented Features</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">The proposed image representation</span></li>
<li><span style="font-size: medium; color: #000000;">Augmenting the local-feature scheme with global image properties so that the vector quantization just described can be used to derive a <em>k</em>-dimensional vector</span></li>
</ul>
</li>
</ul>
<p align="center"><img src="images/research/cbmr/cbmr_3.png" border="0" alt="" /></p>

<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Visualization of Embedding Space</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>The iterative procedure</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Use ARE to compute a 30-D semantic manifold</span></li>
<li><span style="font-size: medium; color: #000000;">Project data points onto a 2-D plane by multidimensional scaling (MDS) [Cox and Cox, 1994] for visualization</span></li>
</ul>
</li>
</ul>
<table style="width: 100%;" border="1">
<tbody>
<tr>
<td align="center"><span style="color: #0000ff;"><img src="images/research/cbmr/cbmr_4.png" border="0" alt="" width="150" /></span>
<p align="center"><span style="color: #0000ff;"><strong><span style="font-size: medium;">Firework</span></strong></span></p>
</td>
<td>
<p align="center"><img src="images/research/cbmr/cbmr_6.gif" border="0" width="700" height="438" /></p>
</td>
</tr>
<tr>
<td align="center"><span style="color: #0000ff;"><img src="images/research/cbmr/cbmr_5.png" border="0" alt="" width="150" /></span>
<p align="center"><span style="color: #0000ff;"><strong><span style="font-size: medium;">Office Interiors</span></strong></span></p>
</td>
<td>　<img src="images/research/cbmr/cbmr_7.gif" border="0" width="700" height="438" style="display: block; margin-left: auto; margin-right: auto;" /></td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>　</p>
<p style="text-align: left"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>

<table border="0" style="text-align: left">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/mm-lin05.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6><span style="line-height: 1.3em;">Semantic Manifold Learning for Image Retrieval</span></h6>
<span style="line-height: 1.3em;">Yen-Yu Lin, Tyng-Luh Liu, and Hwann-Tzong Chen</span><br />
<span style="line-height: 1.3em;"><em>ACM International Conference on Multimedia (ACM MM), December 2005, <span style="color: #00ccff;">(Best Student Papers Session)</span></em></span>
<br />
<a class="readon" href="images/paper/mm-lin05.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 
                                             
                   	 </tbody>
                  </table>

<!--
                    [<a href="" target="_blank">Conference</a>] 
                    [<a href="" target="_blank">Demo</a>]
-->
                    </li>
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
   

    <!-- Modal 1 -->
    <div class="portfolio-modal modal fade" id="Classification" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>Data Classification and Clustering</h3>

<table style="border-collapse: collapse; width: 100%; text-align: justify;" border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td><img src="images/research/dcc/dcc_1.png" border="0" alt="" width="800" style="display: block; margin-left: auto; margin-right: auto;" />
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><span style="font-size: medium; font-weight: bold; color: #000000;">Data, including observations, measurements or images are quantized/characterized with certain feature representations in the digital world for further processing. However, there exists no a universal way to well-depict all the instances. Particularly, the optimal data descriptors often vary from class to class. We are thus motivated to fuse <span style="color: #ff9900;"><em>multiple kernel learning</em> (MKL)</span> into the training procedure, and carry out a class-specific feature selection framework, which significantly facilitates the relevant tasks, such as clustering and classification.</span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">　</p>
<p><span style="font-size: x-large;"><strong><span style="color: #ffcc00;"> Multiple Kernel Learning with Local Learning</span></strong></span></p>
<ul>
<li><span style="font-size: medium; color: #000000;">We address two unfavorable issues of local learning, i.e., high risk of overfitting and heavy computational cost, and present an efficient boosting algorithm to learn sample-specific local classifiers for object category recognition.</span></li>
</ul>
<ul>
<li><span style="color: #000000;"><strong><span style="font-size: medium;">Our approach</span></strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">We cast the multiple, independent training processes of local classifiers as a correlative multi-task learning problem.</span></li>
<li><span style="font-size: medium; color: #000000;">We establish a parametric space where these local classifiers lie and spread as a manifold-like structure.</span></li>
<li><span style="font-size: medium; color: #000000;">By designing a new multi-task boosting algorithm, the local classifiers are obtained by completing the manifold embedding.</span></li>
<li><span style="font-size: medium; color: #000000;">The algorithm carries out incremental multiple kernel learning.</span></li>
</ul>
<p><img src="images/research/dcc/dcc_2.png" border="0" alt="" width="800" style="display: block; margin-left: auto; margin-right: auto;" /></p>
</li>
</ul>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Cluster-dependent Feature Selection</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;">A <strong><em>chicken-and-egg</em></strong> problem.</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Clustering VS. Feature selection: <em><strong>Is it the clustering that contributes to the feature selection, or the feature selection that boosts the clustering?</strong></em></span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;">Again, the optimal data descriptors often vary from cluster to cluster.</span></li>
<li><span style="font-size: medium; color: #000000;">With the idea of associating each cluster with a learnable ensemble kernel, we integrate multiple kernel learning into the clustering procedure, and cast it as a joint optimization problem.</span></li>
</ul>
<p><img src="images/research/dcc/dcc_3.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
</td>
</tr>
</tbody>
</table>

<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>

<table border="0" cellspacing="0" cellpadding="0"  style="text-align: justify;">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/icpr-huang12.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6>Cluster-dependent Feature Selection by Multiple Kernel Self-organizing Map</h6>
Kuan-Chieh Huang, Yen-Yu Lin, and Jie-Zhi Cheng
<br />
<em>IEEE International Conference on Pattern Recognition (ICPR), November 2012</em>
<br />
<a class="readon" href="images/paper/icpr-huang12.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
<tr>
<td valign="top"><img src="images/thumbs/eccv-lin10.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6><span style="line-height: 1.3em;">Clustering Complex Data with Group-dependent Feature Selection</span></h6>
<span style="line-height: 1.3em;">Yen-Yu Lin, Tyng-Luh Liu, and Chiou-Shann Fuh</span>
<br />
<span style="line-height: 1.3em;"> <em>European Conference on Computer Vision (ECCV), September 2010</em></span>
<br>
<a class="readon" href="images/paper/eccv-lin10.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
<tr>
<td valign="top"><img src="images/thumbs/iccv-lin09.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6><span style="line-height: 1.3em;">Efficient Discriminative Local Learning for Object Recognition</span></h6>
<span style="line-height: 1.3em;">Yen-Yu Lin, Jyun-Fan Tsai, and Tyng-Luh Liu</span>
<br />
<span style="line-height: 1.3em;"> <em>IEEE International conference on Computer Vision (ICCV), September 2009</em></span>
<br />
<a class="readon" href="images/paper/iccv-lin09.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 										

                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 2 -->
    <div class="portfolio-modal modal fade" id="FaceDetection" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>Face Detection</h3>


<table style="border-collapse: collapse; width: 100%; text-align: justify;" border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td><img src="images/research/fd/fd_1.png" border="0" alt="" width="800" style="display: block; margin-left: auto; margin-right: auto;" />
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><strong><span style="color: #000000; font-size: medium;">We aim to design a general learning framework for face detection while handling some problems caused by a variety of variations in images, including <span style="color: #ff9900;"><em>Profile, Rotation, Occlusion, Lighting Conditions, Varied Expressions, Multiple Faces</em> and <em>Scales</em></span>. We are motivated to formulate the task as a classification problem over data of multiple classes. Our approach takes advantage of a multi-class boosting algorithm, MBHboost, to effectively perform face detection with the assistance of its integration with a cascade structure. As a result, it features great flexibility in the sense that only one single boosted cascade is needed without worrying about how to select the most appropriate cascade for the detection.</span></strong></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">　</p>
<p><span style="font-size: x-large;"><strong><span style="color: #ffcc00;">Real-time, Multi-view Face Detection</span></strong></span></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong><span style="line-height: 1.3em;">A real-time, multi-view face detector</span></strong></span></li>
<ul>
<li><span style="line-height: 1.3em; font-size: medium; color: #000000;">Multi-view: Profile faces, rotated faces, faces with partial occlusions, or faces under different lighting conditions</span></li>
<li><span style="line-height: 1.3em; font-size: medium; color: #000000;">Real-time: At least <span style="font-weight: bold; color: #ff9900;">15</span> frames per second on a PC</span></li>
</ul>
<li><span style="font-size: medium; color: #000000;"><strong><span style="line-height: 1.3em;">Two key components</span></strong></span></li>
<ul>
<li><span style="line-height: 1.3em; font-size: medium; color: #000000;">A new boosting algorithm: Multi-class Bhattacharyya boost (MBHBoost)</span></li>
<li><span style="line-height: 1.3em; font-size: medium; color: #000000;">A new detection architecture: Multi-class cascade</span></li>
</ul>
<li><span style="line-height: 1.3em; font-size: medium; color: #000000;">The detector leads to a computational cost sub-linear to the number of face classes (views)</span></li>
</ul>
<p><span style="color: #ffcc00;"> </span></p>
<p><strong><span style="color: #ffcc00; font-size: x-large;">Key Features / Techniques</span></strong></p>
<ul>
<li><span style="line-height: 1.3em; font-size: medium; color: #000000;"> <span style="font-weight: bold;">Vector-valued</span> weak learners</span></li>
<li><span style="line-height: 1.3em; font-size: medium; color: #000000;">A new boosting algorithm: Multi-class Bhattacharyya boost (MBHBoost)</span></li>
<li><span style="line-height: 1.3em; font-size: medium; color: #000000;">A new detection architecture: Multi-class cascade</span></li>
</ul>
<p><span style="font-size: x-large; color: #ffcc00;"><strong>Rectangle Features</strong></span></p>
<p><img src="images/research/fd/fd_2.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><span style="color: #ffcc00; font-size: x-large;"><strong> </strong></span></p>
<p><span style="color: #ffcc00; font-size: x-large;"><strong>Problems Caused by Thresholding and Our Solution</strong></span></p>
<p><img src="images/research/fd/fd_3.png" border="0" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><img src="images/research/fd/fd_4.png" border="0" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Classifier Sharing</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;">An vector-valued weak learner associated with is defined by </span><br /><span style="font-size: medium; color: #000000;"> <img src="images/research/fd/fd_5.png" border="0" style="display: block; margin-left: auto; margin-right: auto;" /></span></li>
<li><span style="color: #000000; font-size: medium;"><strong>Advantages</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Shared by all classes, no human knowledge or searching</span></li>
<li><span style="font-size: medium; color: #000000;">Each component independently learns a decision boundary</span></li>
<li><span style="font-size: medium; color: #000000;">Computational efficiency: The value of <em>k</em> is identical in all components</span></li>
</ul>
</li>
</ul>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Multi-class Cascade</span></strong></p>
<p><img src="images/research/fd/fd_6.png" border="0" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<ul>
<li><span style="font-size: medium; color: #000000;">Reduce the detection problem to a series of pattern rejection problem</span></li>
<li><span style="font-size: medium; color: #000000;">Speed up the detection process: Coarse-to-fine detection</span></li>
<li><span style="font-size: medium; color: #000000;">Deal with vector-valued outputs: Message-passing between stages</span></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>

<table border="0" cellspacing="0" cellpadding="10"  style="text-align: justify;">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/cvpr-lin05.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6><span style="line-height: 1.3em;">Robust Face Detection with Multi-class Boosting</span></h6>
<span style="line-height: 1.3em;">Yen-Yu Lin and Tyng-Luh Liu</span>
<br />
<span style="line-height: 1.3em;"> <em>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2005</span>
<br />
<a class="readon" href="images/paper/cvpr-lin05.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
<tr>
<td valign="top"><img src="images/thumbs/eccv-lin04.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6><span style="line-height: 1.3em;">Fast Object Detection with Occlusions</span></h6>
<span style="line-height: 1.3em;">Yen-Yu Lin, Tyng-Luh Liu, and Chiou-Shann Fuh</span>
<br />
<span style="line-height: 1.3em;"> <em>European Conference on Computer Vision (ECCV), May 2004</em></span>
<br />
<a class="readon" href="images/paper/eccv-lin04.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 


                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 3 -->
    <div class="portfolio-modal modal fade" id="FeatureMatching" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>Feature Matching</h3>
				  
<table style="border-collapse: collapse; width: 100%; text-align: justify;" border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td><img src="images/research/fm/fm_1.png" border="0" alt="" width="800" style="display: block; margin-left: auto; margin-right: auto;" />
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><span style="font-size: medium; font-weight: bold; color: #000000;">Feature matching, or feature correspondence serves as a core technique for image analysis and understanding. There is a wide range of applications that are closely related to it, such as object recognition, image retrieval, 3D reconstruction, image enhancement, and so on. The problems of correspondence involves clutter background, significant amount of outliers and occlusion. Moreoever, multiple translations, orientations and deformations also negatively affect the matching of features in terms of precision, recall and efficiency. To this end, we look into these problems and propose robust frameworks to resolve them.</span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">　</p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">What is the problem in feature matching?</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;">Motivation: For image matching, the initial feature correspondence set</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Can not be too large: <strong>Low recall </strong></span></li>
<li><span style="font-size: medium; color: #000000;">Contains corrupt matches: <strong>Low precision</strong></span></li>
<li><span style="font-size: medium; color: #000000;">Requires geometric checking: <strong>Time consuming</strong></span></li>
</ul>
</li>
</ul>
<p><span style="font-size: medium; color: #000000;"><img src="images/research/fm/fm_2.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<ul>
<li><span style="font-size: medium; color: #000000;">Proposed method: Alternate Hough and inverted Hough voting</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Correspondence mutual checking in homography space</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;">Main advantages: </span>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>High precision:</strong> Hough voting</span></li>
<li><span style="font-size: medium; color: #000000;"><strong>High recall:</strong> Inverted Hough voting </span></li>
<li><span style="font-size: medium; color: #000000;">Low computational cost: BPLR for voter filtering</span></li>
</ul>
</li>
</ul>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Core Ideas</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>Precision:</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">The correct matches are biased to a dense cluster in the transformation space</span></li>
<li><span style="font-size: medium; color: #000000;">We cast the task of feature matching problem into a density estimation problem</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Recall:</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Grouped features with high probability undergo similar transformations in matching</span></li>
<li><span style="font-size: medium; color: #000000;">We utilize the nature of BPLR to locate non-crossboundary regions which correspond to groups of similar transformations</span><br /> 　</li>
</ul>
</li>
</ul>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Core </span> <span style="font-size: x-large; color: #ffcc00;">Techniques</span></strong></p>
<ul>
<li><strong><span style="color: #000000; font-size: medium;">Hough voting</span></strong>
<ul>
<li><span style="font-size: medium; color: #000000;">The tentative correspondences are found via nearest-neighbor search in descriptor space and use to generate votes in the transformation space.</span></li>
<li><span style="font-size: medium; color: #000000;">We use density of each correspondence in the transformation space to verify its correctness</span></li>
</ul>
</li>
<li><strong><span style="color: #000000; font-size: medium;">Inverted hough voting</span></strong>
<ul>
<li><span style="color: #000000; font-size: medium;">Recommend each feature additional transformations by investigating density distribution of nearby features covered by the same BPLR.</span></li>
</ul>
</li>
</ul>
<p><span style="font-size: medium;"> <img src="images/research/fm/fm_3.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>
<table border="0" cellspacing="0" cellpadding="10" style="text-align: justify;">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/cvpr-chen13.png" border="0" alt="" width="170" height="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6>Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</h6>
Hsin-Yi Chen, Yen-Yu Lin and Bing-Yu Chen
<br />
<em>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), June 2013</em>
<br />
<a class="readon" href="images/paper/cvpr-chen13.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 			
				  
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Augmented Sensing -->
    <div class="portfolio-modal modal fade" id="ObjectRecognition" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>Object Recognition</h3>
				  
<table style="border-collapse: collapse; width: 100%; text-align: justify; " border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td><img src="images/research/or/or_1.png" border="0" alt="" width="800" style="display: block; margin-left: auto; margin-right: auto;" />
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><span style="font-size: medium; font-weight: bold; color: #000000;">Object recognition is one of the vision applications that deal with data of multiple classes. In such a case, not only discriminative features but also the class-specific features should be considered because the goodness of a feature representation for recognition is often <em><span style="color: #ff9900;">category-dependent</span></em>, and can even be object-dependent for the case of large intra-class variation. Thus, we aim to improve recognition accuracy by focusing on feature representation in terms of image features and similarity measures, where various feature representations in the domain of kernel matrices are fused to alleviate the difficulties caused by diverse forms of them.</span></p>
</td>
</tr>
</tbody>
</table>
<p><span style="font-size: x-large;"><strong><span style="color: #ffcc00;"> Object Recognition with Kernel Machines </span></strong></span></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>The problems and observations</strong></span>
<ul>
<li><span style="font-size: medium; background-color: #ffffff; color: #000000;">Diverse object categories</span></li>
<li><span style="font-size: medium; background-color: #ffffff; color: #000000;">Large intra-class variations</span></li>
<li><span style="font-size: medium; background-color: #ffffff; color: #000000;">The goodness of a feature representation for recognition is often category-dependent, and can even be object-dependent for the case of large intra-class variation</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Improve recognition accuracy by focusing on</strong></span>
<ul>
<li><span style="font-size: medium; background-color: #ffffff; color: #000000;">Image features</span></li>
<li><span style="font-size: medium; background-color: #ffffff; color: #000000;">Similarity measures</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Approach</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Fusing various feature representations in the domain of kernel matrices</span></li>
<li><span style="font-size: medium; color: #000000;">Learning a local ensemble kernel machine (e.g. local ensemble kernel + SVM) for each training sample</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Localized kernel alignment (initialization)</span></li>
<li><span style="font-size: medium; color: #000000;">MRF modeling and optimization</span></li>
<li><span style="font-size: medium; color: #000000;">Associate each sample with a SVM classifier</span></li>
</ul>
</li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Feature Fusion and Kernel Alignment</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Fusing feature representations is now combining kernels</span></li>
<li><span style="font-size: medium; color: #000000;">The effectiveness of a possible fusion can therefore be reasonably estimated by how good an ensemble kernel is</span></li>
<li><span style="font-size: medium; color: #000000;">We consider target kernel alignment for measuring the goodness of a kernel. [Cristianini et al. '01]</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Optimization over a MRF Model</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Retain the effectiveness of each local classifier</span></li>
<li><span style="font-size: medium; color: #000000;">Alleviate the possible overfitting</span></li>
<li><span style="font-size: medium; color: #000000;">Reduce the redundancy of the local classifiers</span></li>
</ul>
</li>
</ul>
<p><img src="images/research/or/or_2.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong style="font-size: 10px; line-height: 1.3em;"><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong style="font-size: 10px; line-height: 1.3em;"><span style="font-size: x-large; color: #ffcc00;">Multiple Kernel Learning (MKL) for Dimensionality Reduction</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>Observations</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">No single feature representation suffices to explain the complexity of the whole data</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Goals</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Perform MKL for heterogeneous feature fusion</span></li>
<li><span style="font-size: medium; color: #000000;">Generalize a set of dimensionality reduction methods to consider multiple kernels</span></li>
<li><span style="font-size: medium; color: #000000;">Extend MKL from supervised learning to unsupervised and semi-supervised learning</span></li>
<li><span style="font-size: medium; color: #000000;">Improve performances of vision applications by using multiple feature representations</span><br /> 　</li>
</ul>
</li>
</ul>
<p><img src="images/research/or/or_3.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>
<table border="0" cellspacing="0" cellpadding="10" style="text-align: justify;">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/pami-lin11.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6>Multiple Kernel Learning for Dimensionality Reduction</h6>
Yen-Yu Lin, Tyng-Luh Liu, and Chiou-Shann Fuh
<br />
<em>IEEE Transction on Pattern Analysis and Machine Intelligence (TPAMI), Vol. 33, No. 6, June 2011</em>
<br />
<a class="readon" href="images/paper/pami-lin11.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
<tr>
<td valign="top"><img src="images/thumbs/nips-lin08.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6><span style="line-height: 1.3em;">Dimensionality Reduction for Data in Multiple Feature Representations</span></h6>
<span style="line-height: 1.3em;">Yen-Yu Lin, Tyng-Luh Liu, and Chiou-Shann Fuh</span>
<br />
<span style="line-height: 1.3em;"> <em>Advances in Neural Information Processing Systems (NIPS), December 2008</em></span>
<br />
<a class="readon" href="images/paper/nips-lin08.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
<tr>
<td valign="top"><img src="images/thumbs/cvpr-lin07.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6><span style="line-height: 1.3em;">Local Ensemble Kernel Learning for Object Category Recognition</span></h6>
<span style="line-height: 1.3em;">Yen-Yu Lin, Tyng-Luh Liu, and Chiou-Shann Fuh</span>
<br />
<span style="line-height: 1.3em;"> <em>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), June 2007</em></span>
<br />
<a class="readon" href="images/paper/cvpr-lin07.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 			
				  
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>   
  
       <!-- SIMTY Wakeup Management -->
    <div class="portfolio-modal modal fade" id="PeopleCounting" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>People Counting</h3>
				  
<table style="border-collapse: collapse; width: 100%; text-align: justify;" border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td><img src="images/research/pc/pc_1.png" border="0" alt="" width="800" style="display: block; margin-left: auto; margin-right: auto;" />
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><span style="font-size: medium; font-weight: bold; color: #000000;">The goal of people counting is to estimate the number of people or the density of crowds in a monitored environment. Both the long-term and short-term statistics of people counts of an environment provide useful information for strategy planning or event detection. However, detecting or estimating the density of crowds is always a challenging task due to some potential difficulties, such as <span style="color: #ff9900;"><em>partial occlusions, low-quality images, clutter backgrounds</em></span>, and so on. To this end, we focus on the framework where multiple cameras with different angles of view are available, and consider the visual cues captured by each camera as a knowledge source, carrying out cross-camera knowledge transfer to alleviate the difficulties.</span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">　</p>
<p><span style="font-size: x-large;"><strong><span style="color: #ffcc00;"> Single-Camera People Counting</span></strong></span></p>
<ul>
<li><span style="color: #000000; font-size: medium;"><strong>Cross-camera people counting</strong></span>
<ul>
<li><span style="color: #000000; font-size: medium;">Applicable to various environments</span></li>
<li><span style="color: #000000; font-size: medium;">Online training data acquisition and camera perspective estimation</span></li>
</ul>
</li>
</ul>
<ul>
<li><span style="color: #000000; font-size: medium;"><strong>Occlusion handling: Coupled Gaussian processes </strong></span>
<ul>
<li><span style="color: #000000; font-size: medium;">First-pass Gaussian processes: Visible part</span></li>
<li><span style="color: #000000; font-size: medium;">Second-pass Gaussian processes: Occluded part</span></li>
</ul>
</li>
</ul>
<p><img src="images/research/pc/pc_2.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Why Predict Conflict Works</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;">An example: prediction conflict between horizontal and vertical gradients for occlusion handling</span></li>
</ul>
<p><span style="font-size: medium;"><img src="images/research/pc/pc_3.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<ul>
<ul>
<li><span style="color: #000000; font-size: medium;"><strong>Legend</strong></span>
<ul>
<li><span style="font-size: medium;"><strong><span style="color: #ff0000;">---- </span></strong> <span style="color: #000000;">Ground truth</span></span></li>
<li><span style="font-size: medium;"><span style="color: #0000ff;"><strong>----</strong> </span> <span style="color: #000000;">Prediction by the feature of</span> <span style="color: #0000ff;"><strong>horizontal</strong></span> <span style="color: #000000;">gradients</span></span></li>
<li><span style="font-size: medium;"><span style="color: #00ff00;"><strong>----</strong> </span> <span style="color: #000000;">Prediction by the feature of</span> <span style="color: #00ff00;"><strong>vertical</strong></span><span style="color: #000000;"> gradients</span></span></li>
</ul>
</li>
</ul>
</ul>
<p> </p>
<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Blob Representation</span></strong></p>
<ul>
<li><span style="color: #000000; font-size: medium;">We focus on foreground objects (pedestrians) in images</span></li>
<li><span style="color: #000000; font-size: medium;">Background subtraction</span></li>
<li><span style="color: #000000; font-size: medium;">Grouping spatially connected pixels</span></li>
</ul>
<p><img src="images/research/pc/pc_4.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p> </p>
<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">People Counting with Multiple Cameras</span></strong></p>
<ul>
<li><span style="color: #000000; font-size: medium;"><strong>Why multiple cameras?</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Complementary information</span></li>
<li><span style="font-size: medium; color: #000000;">Dealing with resolution issues, occlusions, ...</span></li>
</ul>
</li>
</ul>
<ul>
<li><span style="color: #000000; font-size: medium;"><strong>Our approach</strong></span>
<ul>
<li><span style="color: #000000; font-size: medium;">Ground plane matching + Visual knowledge transfer</span></li>
</ul>
</li>
</ul>
<p><img src="images/research/pc/pc_5.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p> </p>
<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Why Bob Matching?</span></strong></p>
<ul>
<li><span style="color: #000000; font-size: medium;">Find the same groups of pedestrians across cameras</span>
<ul>
<li><span style="font-size: medium;"><span style="color: #000000;"><strong>Synchronized frames</strong></span></span>
<ul>
<li><span style="font-size: medium; color: #000000;">The numbers of people are not always equal, particularly when FOVs are quite different</span></li>
</ul>
</li>
<li><span style="color: #000000;"><span style="font-size: medium; line-height: 1.3em;">We work on <strong>corresponding blob sets</strong>:</span><span style="font-size: medium; line-height: 1.3em;"> </span><strong style="font-size: medium; line-height: 1.3em;">blob clusters</strong></span></li>
</ul>
</li>
</ul>
<p><img src="images/research/pc/pc_6.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p> </p>
<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Observation</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;">Approximating the people counts in an image in two parts</span>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>Regular part:</strong> intra-camera visual features</span></li>
<li><span style="font-size: medium; color: #000000;"><strong>Residual part:</strong> inter-camera visual knowledge</span></li>
</ul>
</li>
<li>
<p><span style="color: #000000; font-size: medium;">Formulate it as a transfer learning problem</span></p>
</li>
</ul>
<p><img src="images/research/pc/pc_7.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>
<table border="0" cellspacing="0" cellpadding="10" style="text-align: justify;">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/mm-weng12.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6>Visual Knowledge Transfer among Multiple Cameras for People Counting with Occlusion Handling</h6>
<span>Ming-Fang Weng, Yen-Yu Lin, Nick C. Tang, and Hong-Yuan Mark Liao</span>
<br />
<em>ACM International Conference on Multimedia (MM), October 2012,<span style="color: #00ccff;"> (full paper)</span></em>
<br />
<a class="readon" href="images/paper/mm-weng12.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
<tr>
<td valign="top"><img src="images/thumbs/wifs-lin11.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6><span style="line-height: 1.3em;">Cross Camera People Counting with Perspective Estimation and Occlusion Handling</span></h6>
<span style="line-height: 1.3em;">Tsung-Yi Lin, Yen-Yu Lin, Ming-Fang Weng, Yu-Chiang Wang, Yu-Feng Hsu, and Hong-Yuan Mark Liao</span>
<br />
<span style="line-height: 1.3em;"> <em>IEEE International Workshop on Information Forensics and Security (WIFS), November 2011</em></span>
<br />
<a class="readon" href="images/paper/wifs-lin11.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 			
				  
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  
        <!-- ucsg -->
    <div class="portfolio-modal modal fade" id="SemanticSegmentation" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>Semantic Segmentation</h3>
				  
<table style="border-collapse: collapse; width: 100%; text-align: justify;" border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td><img src="images/research/ss/ss_1.png" border="0" alt="" width="800" style="display: block; margin-left: auto; margin-right: auto;" />
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><span style="font-size: medium; font-weight: bold; color: #000000;"> We focus on semantic segmentation, where class-based image segmentation is of focus as the task of labeling pixels with several pre-defined object classes or background in an image. Distinct from the image driven segmentation task, class based image segmentation aims to not only identify the object classes of interest, but also determine the shapes or boundaries of these objects. It in fact involves resolving two of the most fundamental problems in vision research: </span> <span style="color: #ff9900;"><span style="font-size: medium; font-weight: bold;"> <em>recognition</em></span></span><span style="font-size: medium; font-weight: bold; color: #000000;"> and </span><span style="color: #ff9900;"> <span style="font-size: medium; font-weight: bold;"> <em>segmentation</em></span></span><span style="font-size: medium; font-weight: bold; color: #000000;">. Therefore, it plays an essential role in many high-level computer vision applications, such as image and scene understanding.</span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">　</p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Class-based Image Segmentation</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>A concise annotation method for collecting training data for class based image segmentation. Two steps</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Generate multiple tight segments, combining the multiple segment method with the concept of bounding box prior</span></li>
<li><span style="font-size: medium; color: #000000;">Select the best segment by semi-supervised regression</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Credits</strong></span>
<ul>
<li><span style="font-size: medium; color: #000000;">Present a novel algorithm which integrates the bounding box prior into the concept of multiple image segmentation, and automatically generate multiple tight segments</span></li>
<li><span style="font-size: medium; color: #000000;">Case the segment selection as a semi-supervised regression problem</span></li>
<li><span style="font-size: medium; color: #000000;">Demonstrate that our approach provides an effective alternative for manually labeled contours</span></li>
</ul>
</li>
</ul>
<p><strong><span style="font-size: x-large; color: #ffcc00;"> </span></strong></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Core Techniques</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>Multiple Tight Segment Generation: </strong>Present an algorithm that automatically generates a set of tight segments for the bounding box of an object, and at least one of these tight segments would approach the object segment</span></li>
<li><span style="font-size: medium; color: #000000;"><strong>Segment Selection: </strong>Given a few contours as well as a set of bounding boxes of an object class, we illustrate how to infer the object segments of these bounding boxes by solving a semi-supervised regression problem</span></li>
</ul>
<p><span style="font-size: medium;"><img src="images/research/ss/ss_2.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>
<table border="0" cellspacing="0" cellpadding="10" style="text-align: justify;">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/accv-cheng12.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6>Knowledge Leverage from Contours to Bounding Boxes: A Concise Approach to Annotation</h6>
Jie-Zhi Cheng, Feng-Ju Chang, Kuang-Jui Hsu, and Yen-Yu Lin
<br />
<em>Asian Conference on Computer Vision (ACCV), Lecture Notes in Computer Science, November 2012, Poster presentation (Acceptance rate: 23.2%)</em>
<br />
<a class="readon" href="images/paper/accv-cheng12.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 			
				  
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>       
    <!-- Modal 4 -->
    <div class="portfolio-modal modal fade" id="TransferLearning" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3>Transfer Learning</h3>
				  
<table style="border-collapse: collapse; width: 100%; text-align: justify;" border="0" cellspacing="0" cellpadding="20" bgcolor="#FFFFFF">
<tbody>
<tr>
<td><img src="images/research/tl/tl_1.png" border="0" alt="" width="800" style="display: block; margin-left: auto; margin-right: auto;" />
<table style="width: 100%;" border="0" cellspacing="0" cellpadding="20" bgcolor="#F0F0F0">
<tbody>
<tr>
<td>
<p style="text-align: justify; line-height: 1.5em;"><span style="font-size: medium; font-weight: bold; color: #000000;"> The cost of data labeling for image recognition or classification is often expensive. To reduce the labeling effort, transfer learning has been demonstrated to be a promising technique for object recognition with few training samples. It delivers useful knowledge in the source to improve the target model learning. We particularly focus on transferring knowledge from </span> <span style="color: #ff9900;"><span style="font-size: medium; font-weight: bold;"> <em>multiple classes to multiple classes</em></span></span><span style="font-size: medium; font-weight: bold; color: #000000;">, given two multi-class recognition tasks (one in the </span> <span style="color: #ff9900;"><span style="font-size: medium; font-weight: bold;"> <em>source domain</em></span></span><span style="font-size: medium; font-weight: bold; color: #000000;"> and the other in the </span><span style="color: #ff9900;"> <span style="font-size: medium; font-weight: bold; font-style: italic;"> target domain</span></span><span style="font-size: medium; font-weight: bold; color: #000000;">), leveraging the extra source knowledge to learn a more robust multi-class classifier rather than a set of binary classifiers in the target domain.</span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">　</p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Introduction</span></strong></p>
<ul>
<li><strong><span style="font-size: medium;"><span style="color: #000000;">Multi-class object recognition with</span> <span style="color: #ff9900;">few labeled data</span></span></strong>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>Goal</strong>: Learn a target classifier with low generalization errors</span></li>
<li><span style="font-size: medium; color: #000000;"><strong>Difficulty: </strong>When only few labeled data are available, over-fitting occurs. That is, the yielded classifier has poor generalization.</span></li>
</ul>
</li>
<li><strong><span style="font-size: medium; color: #000000;">What and how prior knowledge help to learn a robust classifier without labeling new data?</span></strong>
<ul>
<li><span style="font-size: medium; color: #000000;">Source task: many existed labeled data</span></li>
<li><span style="font-size: medium; color: #000000;">Target task: few labeled data</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Motivations: </strong>to leverage the extra source knowledge, together with the target knowledge in a common domain, and consequently learn a more robust multi-class classifier</span></li>
<li><span style="font-size: medium; color: #000000;"><strong>Conventional transfer learning algorithms:</strong> lack multi-class formulation</span></li>
</ul>
<p><span style="font-size: medium;"> <img src="images/research/tl/tl_2.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><strong><span style="font-size: x-large; color: #ffcc00;">Core </span> <span style="font-size: x-large; color: #ffcc00;">Ideas</span></strong></p>
<ul>
<li><span style="font-size: medium; color: #000000;">Attribute transfer</span></li>
<li><span style="font-size: medium; color: #000000;">Multi-classes (source) to multi-classes (target) knowledge transfer</span></li>
<li><span style="font-size: medium; color: #000000;"><strong>What to transfer: </strong>a sequence of learnable, discriminant attributes</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Commonly shared by the source and target domains</span></li>
<li><span style="font-size: medium; color: #000000;">Converted two multi-class classification tasks to related, binary ones</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>How to transfer: </strong>Two-layer multi-task variant of AdaBoost.OC</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Boosting algorithm with error-correcting output codes (ECOC)</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Better generalization</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Outer layer: </strong>Attribute partition discovery</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Discriminant:Multi-class formulation</span></li>
<li><span style="font-size: medium; color: #000000;">Learnable:Without human effort</span></li>
<li><span style="font-size: medium; color: #000000;">Complementary:Iterative error minimization</span></li>
</ul>
</li>
<li><span style="font-size: medium; color: #000000;"><strong>Inner layer: </strong>Attribute classifier learning</span>
<ul>
<li><span style="font-size: medium; color: #000000;">Employ classifier sharing principle</span></li>
<li><span style="font-size: medium; color: #000000;">Support multiple kernel learning: Combining various low-level features</span></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><span style="font-size: medium;"> <img src="images/research/tl/tl_3.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<ul>
<li><span style="font-size: medium; color: #000000;"><strong>Our goal: </strong>Attributes should be learnable</span></li>
</ul>
<p><span style="font-size: medium;"> <img src="images/research/tl/tl_4.png" border="0" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
</td>
</tr>
</tbody>
</table>
<p style="text-align: justify;"><strong><span style="font-size: x-large; color: #ffcc00;">Publications</span></strong></p>
<table border="0" cellspacing="0" cellpadding="10" style="text-align: justify;">
<tbody>
<tr>
<td valign="top"><img src="images/thumbs/accv-chang12.png" border="0" alt="" width="170" /></td>
<td valign="top">　</td>
<td valign="top">
<h6>Cross-Database Transfer Learning via Learnable and Discriminant Error-correcting Output Codes</h6>
Feng-Ju Chang, Yen-Yu Lin, and Ming-Fang Weng
<br />
<em>Asian Conference on Computer Vision (ACCV), Lecture Notes in Computer Science, November 2012,<span style="color: #00ccff;"> (Oral presentation; Acceptance rate 3.6% (31/869))</span></em>
<br />
<a class="readon" href="images/paper/accv-chang12.pdf" target="_blank"><span>Paper</span></a>
</td>
</tr>
</tbody>
</table> 				
				  
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fa fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>


	<!-- Projocrs Part2 Modals  Projocrs Part2 Modals  Projocrs Part2 Modals  Projocrs Part2 Modals -->
    <!-- Bootstrap core JavaScript -->
    <script src="./vendor/jquery/jquery.min.js"></script>
    <script src="./vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="./vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Contact form JavaScript -->
    <script src="./js/jqBootstrapValidation.js"></script>
    <script src="./js/contact_me.js"></script>

    <!-- Custom scripts for this template -->
    <script src="./js/agency.min.js"></script>
    
	<script src="./js/jquery.magnific-popup.js" type="text/javascript"></script>
	<script type="text/javascript" src="./js/modernizr.custom.53451.js"></script> 
	<script src="https://momentjs.com/downloads/moment.min.js"></script>
	<script src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
	
	<script type="text/javascript">
    	$(document).ready(function() {
			var bgImgUrl = 'img/bg{num}.jpg', bgNum,bgImgArr = [];
			for (var i=1; i <= 4; i++){
				bgImgArr.push(bgImgUrl.replace('{num}', i));
			}
//			$('header.masthead').css('background-image', 'url(img/bg3.jpg)');
			function displayTime() {
				if(!bgNum || bgNum >= bgImgArr.length) bgNum = 0;
				$('header.masthead').css('background-image', 'url('+ bgImgArr[bgNum] +')');
				bgNum++;
			}
			// Runs the displayTime function the first time
			displayTime();
			// Runs the displayTime function every second.
			setInterval(displayTime, 99000);
    	});
	</script>
 	<script>
		$(document).ready(function() {
		$('.popup-with-zoom-anim').magnificPopup({
			type: 'inline',
			fixedContentPos: false,
			fixedBgPos: true,
			overflowY: 'auto',
			closeBtnInside: true,
			preloader: false,
			midClick: true,
			removalDelay: 300,
			mainClass: 'my-mfp-zoom-in'
		});

		});
	</script>
	<script src="./js/bibtex_js.js?v=2"></script>
	<bibtex src="publications_list.bib?v=20180628"></bibtex>
	<!--  Table -->
	<script type="./application/x-javascript"> addEventListener("load", function() { setTimeout(hideURLbar, 0); }, false); function hideURLbar(){ window.scrollTo(0,1); } </script>
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
	<!--webfonts-->
	<link href="https://fonts.googleapis.com/css?family=Lato:100,300,400,700,900,100italic,300italic,400italic,700italic" rel="stylesheet" type="text/css">
	<script src="./js/jquery.magnific-popup.js"></script>
	<script src="./js/modernizr.custom.53451.js"></script>
 	
<!--  	<bibtex src="publications.bib"></bibtex>-->
 
  </body>

</html>
